{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 12:18:37.594006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 12:18:37.653603: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-27 12:18:37.670932: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 12:18:37.989360: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:\n",
      "2023-12-27 12:18:37.989480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:\n",
      "2023-12-27 12:18:37.989485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer, LSTM\n",
    "from tensorflow.keras.optimizers import  Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "#Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>That's What I Like (feat. Gucci Mane)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>134.071</td>\n",
       "      <td>234596.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston</td>\n",
       "      <td>Hitch a Ride</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.814</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>116.454</td>\n",
       "      <td>251733.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Raincoats</td>\n",
       "      <td>No Side to Fall In</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.334</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>147.681</td>\n",
       "      <td>109667.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deno</td>\n",
       "      <td>Lingo (feat. J.I &amp; Chunkz)</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.597</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.528</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>107.033</td>\n",
       "      <td>173968.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Nobody Weird Like Me - Remastered</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.975</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>199.060</td>\n",
       "      <td>229960.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>Green-House</td>\n",
       "      <td>Find Home</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.109</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>171.587</td>\n",
       "      <td>193450.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>Micatone</td>\n",
       "      <td>All Gone</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.223</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-10.174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>73.016</td>\n",
       "      <td>257067.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>Smash Hit Combo</td>\n",
       "      <td>Peine perdue</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.981</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.683</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>105.000</td>\n",
       "      <td>216222.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>Beherit</td>\n",
       "      <td>Salomon's Gate</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.805</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-12.757</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>131.363</td>\n",
       "      <td>219693.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>The Raconteurs</td>\n",
       "      <td>Broken Boy Soldier</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.853</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.3770</td>\n",
       "      <td>138.102</td>\n",
       "      <td>182227.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17996 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Artist Name                             Track Name  \\\n",
       "0                 Bruno Mars  That's What I Like (feat. Gucci Mane)   \n",
       "1                     Boston                           Hitch a Ride   \n",
       "2              The Raincoats                     No Side to Fall In   \n",
       "3                       Deno             Lingo (feat. J.I & Chunkz)   \n",
       "4      Red Hot Chili Peppers      Nobody Weird Like Me - Remastered   \n",
       "...                      ...                                    ...   \n",
       "17991            Green-House                              Find Home   \n",
       "17992               Micatone                               All Gone   \n",
       "17993        Smash Hit Combo                           Peine perdue   \n",
       "17994                Beherit                         Salomon's Gate   \n",
       "17995         The Raconteurs                     Broken Boy Soldier   \n",
       "\n",
       "       Popularity  danceability  energy   key  loudness  mode  speechiness  \\\n",
       "0            60.0         0.854   0.564   1.0    -4.964     1       0.0485   \n",
       "1            54.0         0.382   0.814   3.0    -7.230     1       0.0406   \n",
       "2            35.0         0.434   0.614   6.0    -8.334     1       0.0525   \n",
       "3            66.0         0.853   0.597  10.0    -6.528     0       0.0555   \n",
       "4            53.0         0.167   0.975   2.0    -4.279     1       0.2160   \n",
       "...           ...           ...     ...   ...       ...   ...          ...   \n",
       "17991        35.0         0.166   0.109   7.0   -17.100     0       0.0413   \n",
       "17992        27.0         0.638   0.223  11.0   -10.174     0       0.0329   \n",
       "17993        34.0         0.558   0.981   4.0    -4.683     0       0.0712   \n",
       "17994        29.0         0.215   0.805   6.0   -12.757     0       0.1340   \n",
       "17995        43.0         0.400   0.853   4.0    -5.320     0       0.0591   \n",
       "\n",
       "       acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0          0.017100               NaN    0.0849   0.8990  134.071   \n",
       "1          0.001100          0.004010    0.1010   0.5690  116.454   \n",
       "2          0.486000          0.000196    0.3940   0.7870  147.681   \n",
       "3          0.021200               NaN    0.1220   0.5690  107.033   \n",
       "4          0.000169          0.016100    0.1720   0.0918  199.060   \n",
       "...             ...               ...       ...      ...      ...   \n",
       "17991      0.993000          0.824000    0.0984   0.1770  171.587   \n",
       "17992      0.858000          0.000016    0.0705   0.3350   73.016   \n",
       "17993      0.000030          0.000136    0.6660   0.2620  105.000   \n",
       "17994      0.001290          0.916000    0.2560   0.3550  131.363   \n",
       "17995      0.006040          0.212000    0.3340   0.3770  138.102   \n",
       "\n",
       "       duration_in min/ms  time_signature  Class  \n",
       "0                234596.0               4      5  \n",
       "1                251733.0               4     10  \n",
       "2                109667.0               4      6  \n",
       "3                173968.0               4      5  \n",
       "4                229960.0               4     10  \n",
       "...                   ...             ...    ...  \n",
       "17991            193450.0               3      6  \n",
       "17992            257067.0               4      2  \n",
       "17993            216222.0               4      8  \n",
       "17994            219693.0               4      8  \n",
       "17995            182227.0               4     10  \n",
       "\n",
       "[17996 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>That's What I Like (feat. Gucci Mane)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.964</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>134.071</td>\n",
       "      <td>234596.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston</td>\n",
       "      <td>Hitch a Ride</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.814</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>116.454</td>\n",
       "      <td>251733.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Raincoats</td>\n",
       "      <td>No Side to Fall In</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.334</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>147.681</td>\n",
       "      <td>109667.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deno</td>\n",
       "      <td>Lingo (feat. J.I &amp; Chunkz)</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.597</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-6.528</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>107.033</td>\n",
       "      <td>173968.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Nobody Weird Like Me - Remastered</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.975</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>199.060</td>\n",
       "      <td>229960.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Artist Name                             Track Name  Popularity  \\\n",
       "0             Bruno Mars  That's What I Like (feat. Gucci Mane)        60.0   \n",
       "1                 Boston                           Hitch a Ride        54.0   \n",
       "2          The Raincoats                     No Side to Fall In        35.0   \n",
       "3                   Deno             Lingo (feat. J.I & Chunkz)        66.0   \n",
       "4  Red Hot Chili Peppers      Nobody Weird Like Me - Remastered        53.0   \n",
       "\n",
       "   danceability  energy   key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.854   0.564   1.0    -4.964     1       0.0485      0.017100   \n",
       "1         0.382   0.814   3.0    -7.230     1       0.0406      0.001100   \n",
       "2         0.434   0.614   6.0    -8.334     1       0.0525      0.486000   \n",
       "3         0.853   0.597  10.0    -6.528     0       0.0555      0.021200   \n",
       "4         0.167   0.975   2.0    -4.279     1       0.2160      0.000169   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_in min/ms  \\\n",
       "0               NaN    0.0849   0.8990  134.071            234596.0   \n",
       "1          0.004010    0.1010   0.5690  116.454            251733.0   \n",
       "2          0.000196    0.3940   0.7870  147.681            109667.0   \n",
       "3               NaN    0.1220   0.5690  107.033            173968.0   \n",
       "4          0.016100    0.1720   0.0918  199.060            229960.0   \n",
       "\n",
       "   time_signature  Class  \n",
       "0               4      5  \n",
       "1               4     10  \n",
       "2               4      6  \n",
       "3               4      5  \n",
       "4               4     10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Artist Name              0\n",
       "Track Name               0\n",
       "Popularity             428\n",
       "danceability             0\n",
       "energy                   0\n",
       "key                   2014\n",
       "loudness                 0\n",
       "mode                     0\n",
       "speechiness              0\n",
       "acousticness             0\n",
       "instrumentalness      4377\n",
       "liveness                 0\n",
       "valence                  0\n",
       "tempo                    0\n",
       "duration_in min/ms       0\n",
       "time_signature           0\n",
       "Class                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisar valores nulos\n",
    "df_nulls = df.isnull().sum()\n",
    "df_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston</td>\n",
       "      <td>Hitch a Ride</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.814</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>116.454</td>\n",
       "      <td>251733.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Raincoats</td>\n",
       "      <td>No Side to Fall In</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.334</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>147.681</td>\n",
       "      <td>109667.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Nobody Weird Like Me - Remastered</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.975</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>199.060</td>\n",
       "      <td>229960.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Stooges</td>\n",
       "      <td>Search and Destroy - Iggy Pop Mix</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.977</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.878</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>152.952</td>\n",
       "      <td>208133.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Solomon Burke</td>\n",
       "      <td>None Of Us Are Free</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.658</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-9.647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>143.292</td>\n",
       "      <td>329387.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>Green-House</td>\n",
       "      <td>Find Home</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.109</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>171.587</td>\n",
       "      <td>193450.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>Micatone</td>\n",
       "      <td>All Gone</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.223</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-10.174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>73.016</td>\n",
       "      <td>257067.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>Smash Hit Combo</td>\n",
       "      <td>Peine perdue</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.981</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.683</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>105.000</td>\n",
       "      <td>216222.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>Beherit</td>\n",
       "      <td>Salomon's Gate</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.805</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-12.757</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>131.363</td>\n",
       "      <td>219693.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>The Raconteurs</td>\n",
       "      <td>Broken Boy Soldier</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.853</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.3770</td>\n",
       "      <td>138.102</td>\n",
       "      <td>182227.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11813 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Artist Name                         Track Name  Popularity  \\\n",
       "1                     Boston                       Hitch a Ride        54.0   \n",
       "2              The Raincoats                 No Side to Fall In        35.0   \n",
       "4      Red Hot Chili Peppers  Nobody Weird Like Me - Remastered        53.0   \n",
       "5                The Stooges  Search and Destroy - Iggy Pop Mix        53.0   \n",
       "6              Solomon Burke                None Of Us Are Free        48.0   \n",
       "...                      ...                                ...         ...   \n",
       "17991            Green-House                          Find Home        35.0   \n",
       "17992               Micatone                           All Gone        27.0   \n",
       "17993        Smash Hit Combo                       Peine perdue        34.0   \n",
       "17994                Beherit                     Salomon's Gate        29.0   \n",
       "17995         The Raconteurs                 Broken Boy Soldier        43.0   \n",
       "\n",
       "       danceability  energy   key  loudness  mode  speechiness  acousticness  \\\n",
       "1             0.382   0.814   3.0    -7.230     1       0.0406      0.001100   \n",
       "2             0.434   0.614   6.0    -8.334     1       0.0525      0.486000   \n",
       "4             0.167   0.975   2.0    -4.279     1       0.2160      0.000169   \n",
       "5             0.235   0.977   6.0     0.878     1       0.1070      0.003530   \n",
       "6             0.674   0.658   5.0    -9.647     0       0.1040      0.404000   \n",
       "...             ...     ...   ...       ...   ...          ...           ...   \n",
       "17991         0.166   0.109   7.0   -17.100     0       0.0413      0.993000   \n",
       "17992         0.638   0.223  11.0   -10.174     0       0.0329      0.858000   \n",
       "17993         0.558   0.981   4.0    -4.683     0       0.0712      0.000030   \n",
       "17994         0.215   0.805   6.0   -12.757     0       0.1340      0.001290   \n",
       "17995         0.400   0.853   4.0    -5.320     0       0.0591      0.006040   \n",
       "\n",
       "       instrumentalness  liveness  valence    tempo  duration_in min/ms  \\\n",
       "1              0.004010    0.1010   0.5690  116.454            251733.0   \n",
       "2              0.000196    0.3940   0.7870  147.681            109667.0   \n",
       "4              0.016100    0.1720   0.0918  199.060            229960.0   \n",
       "5              0.006040    0.1720   0.2410  152.952            208133.0   \n",
       "6              0.000001    0.0981   0.6770  143.292            329387.0   \n",
       "...                 ...       ...      ...      ...                 ...   \n",
       "17991          0.824000    0.0984   0.1770  171.587            193450.0   \n",
       "17992          0.000016    0.0705   0.3350   73.016            257067.0   \n",
       "17993          0.000136    0.6660   0.2620  105.000            216222.0   \n",
       "17994          0.916000    0.2560   0.3550  131.363            219693.0   \n",
       "17995          0.212000    0.3340   0.3770  138.102            182227.0   \n",
       "\n",
       "       time_signature  Class  \n",
       "1                   4     10  \n",
       "2                   4      6  \n",
       "4                   4     10  \n",
       "5                   4      6  \n",
       "6                   4      2  \n",
       "...               ...    ...  \n",
       "17991               3      6  \n",
       "17992               4      2  \n",
       "17993               4      8  \n",
       "17994               4      8  \n",
       "17995               4     10  \n",
       "\n",
       "[11813 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar filas con valores faltantes\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.814</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>116.454</td>\n",
       "      <td>251733.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.334</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>147.681</td>\n",
       "      <td>109667.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.975</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>199.060</td>\n",
       "      <td>229960.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.977</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.878</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>152.952</td>\n",
       "      <td>208133.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.658</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-9.647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>143.292</td>\n",
       "      <td>329387.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.109</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>171.587</td>\n",
       "      <td>193450.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.223</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-10.174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>73.016</td>\n",
       "      <td>257067.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.981</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.683</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>105.000</td>\n",
       "      <td>216222.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.805</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-12.757</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>131.363</td>\n",
       "      <td>219693.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.853</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.3770</td>\n",
       "      <td>138.102</td>\n",
       "      <td>182227.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11813 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Popularity  danceability  energy   key  loudness  mode  speechiness  \\\n",
       "1            54.0         0.382   0.814   3.0    -7.230     1       0.0406   \n",
       "2            35.0         0.434   0.614   6.0    -8.334     1       0.0525   \n",
       "4            53.0         0.167   0.975   2.0    -4.279     1       0.2160   \n",
       "5            53.0         0.235   0.977   6.0     0.878     1       0.1070   \n",
       "6            48.0         0.674   0.658   5.0    -9.647     0       0.1040   \n",
       "...           ...           ...     ...   ...       ...   ...          ...   \n",
       "17991        35.0         0.166   0.109   7.0   -17.100     0       0.0413   \n",
       "17992        27.0         0.638   0.223  11.0   -10.174     0       0.0329   \n",
       "17993        34.0         0.558   0.981   4.0    -4.683     0       0.0712   \n",
       "17994        29.0         0.215   0.805   6.0   -12.757     0       0.1340   \n",
       "17995        43.0         0.400   0.853   4.0    -5.320     0       0.0591   \n",
       "\n",
       "       acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "1          0.001100          0.004010    0.1010   0.5690  116.454   \n",
       "2          0.486000          0.000196    0.3940   0.7870  147.681   \n",
       "4          0.000169          0.016100    0.1720   0.0918  199.060   \n",
       "5          0.003530          0.006040    0.1720   0.2410  152.952   \n",
       "6          0.404000          0.000001    0.0981   0.6770  143.292   \n",
       "...             ...               ...       ...      ...      ...   \n",
       "17991      0.993000          0.824000    0.0984   0.1770  171.587   \n",
       "17992      0.858000          0.000016    0.0705   0.3350   73.016   \n",
       "17993      0.000030          0.000136    0.6660   0.2620  105.000   \n",
       "17994      0.001290          0.916000    0.2560   0.3550  131.363   \n",
       "17995      0.006040          0.212000    0.3340   0.3770  138.102   \n",
       "\n",
       "       duration_in min/ms  time_signature  Class  \n",
       "1                251733.0               4     10  \n",
       "2                109667.0               4      6  \n",
       "4                229960.0               4     10  \n",
       "5                208133.0               4      6  \n",
       "6                329387.0               4      2  \n",
       "...                   ...             ...    ...  \n",
       "17991            193450.0               3      6  \n",
       "17992            257067.0               4      2  \n",
       "17993            216222.0               4      8  \n",
       "17994            219693.0               4      8  \n",
       "17995            182227.0               4     10  \n",
       "\n",
       "[11813 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar las columnas 'Artist Name' y 'Track Name'\n",
    "df = df.drop(['Artist Name', 'Track Name'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.657863</td>\n",
       "      <td>-0.861080</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.232529</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.450090</td>\n",
       "      <td>-0.748501</td>\n",
       "      <td>-0.575982</td>\n",
       "      <td>-0.589070</td>\n",
       "      <td>0.404232</td>\n",
       "      <td>-0.220004</td>\n",
       "      <td>0.334812</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.488940</td>\n",
       "      <td>-0.547837</td>\n",
       "      <td>-0.232364</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.027879</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.274522</td>\n",
       "      <td>0.770948</td>\n",
       "      <td>-0.588537</td>\n",
       "      <td>1.245203</td>\n",
       "      <td>1.295395</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>-0.891468</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.597505</td>\n",
       "      <td>-2.156218</td>\n",
       "      <td>1.257441</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.928601</td>\n",
       "      <td>1</td>\n",
       "      <td>2.137688</td>\n",
       "      <td>-0.751419</td>\n",
       "      <td>-0.536186</td>\n",
       "      <td>-0.144588</td>\n",
       "      <td>-1.546515</td>\n",
       "      <td>2.586894</td>\n",
       "      <td>0.146873</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.597505</td>\n",
       "      <td>-1.746593</td>\n",
       "      <td>1.265694</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.145018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529548</td>\n",
       "      <td>-0.740887</td>\n",
       "      <td>-0.569300</td>\n",
       "      <td>-0.144588</td>\n",
       "      <td>-0.936600</td>\n",
       "      <td>1.020174</td>\n",
       "      <td>-0.041532</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.295715</td>\n",
       "      <td>0.897898</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.337585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485287</td>\n",
       "      <td>0.513999</td>\n",
       "      <td>-0.589178</td>\n",
       "      <td>-0.607225</td>\n",
       "      <td>0.845726</td>\n",
       "      <td>0.691934</td>\n",
       "      <td>1.005103</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Popularity  danceability    energy  key  loudness  mode  speechiness  \\\n",
       "1    0.657863     -0.861080  0.593013  3.0  0.232529     1    -0.450090   \n",
       "2   -0.488940     -0.547837 -0.232364  6.0 -0.027879     1    -0.274522   \n",
       "4    0.597505     -2.156218  1.257441  2.0  0.928601     1     2.137688   \n",
       "5    0.597505     -1.746593  1.265694  6.0  2.145018     1     0.529548   \n",
       "6    0.295715      0.897898 -0.050781  5.0 -0.337585     0     0.485287   \n",
       "\n",
       "   acousticness  instrumentalness  liveness   valence     tempo  \\\n",
       "1     -0.748501         -0.575982 -0.589070  0.404232 -0.220004   \n",
       "2      0.770948         -0.588537  1.245203  1.295395  0.841069   \n",
       "4     -0.751419         -0.536186 -0.144588 -1.546515  2.586894   \n",
       "5     -0.740887         -0.569300 -0.144588 -0.936600  1.020174   \n",
       "6      0.513999         -0.589178 -0.607225  0.845726  0.691934   \n",
       "\n",
       "   duration_in min/ms  time_signature  Class  \n",
       "1            0.334812               4     10  \n",
       "2           -0.891468               4      6  \n",
       "4            0.146873               4     10  \n",
       "5           -0.041532               4      6  \n",
       "6            1.005103               4      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estandarización de los datos\n",
    "valores_numericos = df.columns.drop(['key', 'mode', 'time_signature', 'Class'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[valores_numericos] = scaler.fit_transform(df[valores_numericos])\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar target y features\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_in min/ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.657863</td>\n",
       "      <td>-0.861080</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.232529</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.450090</td>\n",
       "      <td>-0.748501</td>\n",
       "      <td>-0.575982</td>\n",
       "      <td>-0.589070</td>\n",
       "      <td>0.404232</td>\n",
       "      <td>-0.220004</td>\n",
       "      <td>0.334812</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.488940</td>\n",
       "      <td>-0.547837</td>\n",
       "      <td>-0.232364</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.027879</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.274522</td>\n",
       "      <td>0.770948</td>\n",
       "      <td>-0.588537</td>\n",
       "      <td>1.245203</td>\n",
       "      <td>1.295395</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>-0.891468</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.597505</td>\n",
       "      <td>-2.156218</td>\n",
       "      <td>1.257441</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.928601</td>\n",
       "      <td>1</td>\n",
       "      <td>2.137688</td>\n",
       "      <td>-0.751419</td>\n",
       "      <td>-0.536186</td>\n",
       "      <td>-0.144588</td>\n",
       "      <td>-1.546515</td>\n",
       "      <td>2.586894</td>\n",
       "      <td>0.146873</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.597505</td>\n",
       "      <td>-1.746593</td>\n",
       "      <td>1.265694</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.145018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529548</td>\n",
       "      <td>-0.740887</td>\n",
       "      <td>-0.569300</td>\n",
       "      <td>-0.144588</td>\n",
       "      <td>-0.936600</td>\n",
       "      <td>1.020174</td>\n",
       "      <td>-0.041532</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.295715</td>\n",
       "      <td>0.897898</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.337585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485287</td>\n",
       "      <td>0.513999</td>\n",
       "      <td>-0.589178</td>\n",
       "      <td>-0.607225</td>\n",
       "      <td>0.845726</td>\n",
       "      <td>0.691934</td>\n",
       "      <td>1.005103</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>-0.488940</td>\n",
       "      <td>-2.162242</td>\n",
       "      <td>-2.316438</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.095575</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.439762</td>\n",
       "      <td>2.359649</td>\n",
       "      <td>2.123160</td>\n",
       "      <td>-0.605347</td>\n",
       "      <td>-1.198226</td>\n",
       "      <td>1.653379</td>\n",
       "      <td>-0.168273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>-0.971804</td>\n",
       "      <td>0.681038</td>\n",
       "      <td>-1.845974</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.461892</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.563692</td>\n",
       "      <td>1.936622</td>\n",
       "      <td>-0.589129</td>\n",
       "      <td>-0.780010</td>\n",
       "      <td>-0.552337</td>\n",
       "      <td>-1.695999</td>\n",
       "      <td>0.380854</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>-0.549298</td>\n",
       "      <td>0.199126</td>\n",
       "      <td>1.282202</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.833307</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.751855</td>\n",
       "      <td>-0.588734</td>\n",
       "      <td>2.948009</td>\n",
       "      <td>-0.850754</td>\n",
       "      <td>-0.609204</td>\n",
       "      <td>0.028290</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>-0.851088</td>\n",
       "      <td>-1.867071</td>\n",
       "      <td>0.555871</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.071162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.927895</td>\n",
       "      <td>-0.747906</td>\n",
       "      <td>2.425994</td>\n",
       "      <td>0.381279</td>\n",
       "      <td>-0.470579</td>\n",
       "      <td>0.286594</td>\n",
       "      <td>0.058251</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>-0.006075</td>\n",
       "      <td>-0.752650</td>\n",
       "      <td>0.753961</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.683053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.177148</td>\n",
       "      <td>-0.733022</td>\n",
       "      <td>0.108654</td>\n",
       "      <td>0.869584</td>\n",
       "      <td>-0.380645</td>\n",
       "      <td>0.515581</td>\n",
       "      <td>-0.265147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11813 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Popularity  danceability    energy   key  loudness  mode  speechiness  \\\n",
       "1        0.657863     -0.861080  0.593013   3.0  0.232529     1    -0.450090   \n",
       "2       -0.488940     -0.547837 -0.232364   6.0 -0.027879     1    -0.274522   \n",
       "4        0.597505     -2.156218  1.257441   2.0  0.928601     1     2.137688   \n",
       "5        0.597505     -1.746593  1.265694   6.0  2.145018     1     0.529548   \n",
       "6        0.295715      0.897898 -0.050781   5.0 -0.337585     0     0.485287   \n",
       "...           ...           ...       ...   ...       ...   ...          ...   \n",
       "17991   -0.488940     -2.162242 -2.316438   7.0 -2.095575     0    -0.439762   \n",
       "17992   -0.971804      0.681038 -1.845974  11.0 -0.461892     0    -0.563692   \n",
       "17993   -0.549298      0.199126  1.282202   4.0  0.833307     0     0.001370   \n",
       "17994   -0.851088     -1.867071  0.555871   6.0 -1.071162     0     0.927895   \n",
       "17995   -0.006075     -0.752650  0.753961   4.0  0.683053     0    -0.177148   \n",
       "\n",
       "       acousticness  instrumentalness  liveness   valence     tempo  \\\n",
       "1         -0.748501         -0.575982 -0.589070  0.404232 -0.220004   \n",
       "2          0.770948         -0.588537  1.245203  1.295395  0.841069   \n",
       "4         -0.751419         -0.536186 -0.144588 -1.546515  2.586894   \n",
       "5         -0.740887         -0.569300 -0.144588 -0.936600  1.020174   \n",
       "6          0.513999         -0.589178 -0.607225  0.845726  0.691934   \n",
       "...             ...               ...       ...       ...       ...   \n",
       "17991      2.359649          2.123160 -0.605347 -1.198226  1.653379   \n",
       "17992      1.936622         -0.589129 -0.780010 -0.552337 -1.695999   \n",
       "17993     -0.751855         -0.588734  2.948009 -0.850754 -0.609204   \n",
       "17994     -0.747906          2.425994  0.381279 -0.470579  0.286594   \n",
       "17995     -0.733022          0.108654  0.869584 -0.380645  0.515581   \n",
       "\n",
       "       duration_in min/ms  time_signature  \n",
       "1                0.334812               4  \n",
       "2               -0.891468               4  \n",
       "4                0.146873               4  \n",
       "5               -0.041532               4  \n",
       "6                1.005103               4  \n",
       "...                   ...             ...  \n",
       "17991           -0.168273               3  \n",
       "17992            0.380854               4  \n",
       "17993            0.028290               4  \n",
       "17994            0.058251               4  \n",
       "17995           -0.265147               4  \n",
       "\n",
       "[11813 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=256, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4748201438848921"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = DecisionTreeClassifier(random_state=42)\n",
    "dtc_model.fit(X_train, y_train)\n",
    "y_pred = dtc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3406686415573424"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = scaler.fit_transform(X_train)\n",
    "X_test_t = scaler.transform(X_test)\n",
    "y_train_t = to_categorical(y_train)\n",
    "y_test_t = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "296/296 [==============================] - 0s 1000us/step - loss: 1.5712 - accuracy: 0.4002 - val_loss: 1.4123 - val_accuracy: 0.4537\n",
      "Epoch 2/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.4300 - accuracy: 0.4453 - val_loss: 1.3663 - val_accuracy: 0.4520\n",
      "Epoch 3/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.4094 - accuracy: 0.4538 - val_loss: 1.3682 - val_accuracy: 0.4583\n",
      "Epoch 4/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.3897 - accuracy: 0.4547 - val_loss: 1.3912 - val_accuracy: 0.4736\n",
      "Epoch 5/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.3922 - accuracy: 0.4610 - val_loss: 1.3671 - val_accuracy: 0.4723\n",
      "Epoch 6/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.3760 - accuracy: 0.4692 - val_loss: 1.3671 - val_accuracy: 0.4634\n",
      "Epoch 7/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.3721 - accuracy: 0.4693 - val_loss: 1.3619 - val_accuracy: 0.4655\n",
      "Epoch 8/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.3685 - accuracy: 0.4686 - val_loss: 1.3659 - val_accuracy: 0.4566\n",
      "Epoch 9/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.3581 - accuracy: 0.4673 - val_loss: 1.3386 - val_accuracy: 0.4841\n",
      "Epoch 10/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.3648 - accuracy: 0.4737 - val_loss: 1.3360 - val_accuracy: 0.4693\n",
      "Epoch 11/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.3544 - accuracy: 0.4789 - val_loss: 1.3486 - val_accuracy: 0.4791\n",
      "Epoch 12/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.3446 - accuracy: 0.4823 - val_loss: 1.3347 - val_accuracy: 0.4820\n",
      "Epoch 13/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.3455 - accuracy: 0.4748 - val_loss: 1.3445 - val_accuracy: 0.4659\n",
      "Epoch 14/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.3443 - accuracy: 0.4772 - val_loss: 1.3447 - val_accuracy: 0.4769\n",
      "Epoch 15/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.3418 - accuracy: 0.4814 - val_loss: 1.3396 - val_accuracy: 0.4731\n",
      "Epoch 16/1000\n",
      "296/296 [==============================] - 0s 709us/step - loss: 1.3409 - accuracy: 0.4782 - val_loss: 1.3426 - val_accuracy: 0.4727\n",
      "Epoch 17/1000\n",
      "296/296 [==============================] - 0s 709us/step - loss: 1.3406 - accuracy: 0.4830 - val_loss: 1.3347 - val_accuracy: 0.4816\n",
      "Epoch 18/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.3358 - accuracy: 0.4795 - val_loss: 1.3312 - val_accuracy: 0.4829\n",
      "Epoch 19/1000\n",
      "296/296 [==============================] - 0s 707us/step - loss: 1.3287 - accuracy: 0.4823 - val_loss: 1.3165 - val_accuracy: 0.4939\n",
      "Epoch 20/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.3238 - accuracy: 0.4906 - val_loss: 1.3220 - val_accuracy: 0.4901\n",
      "Epoch 21/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.3264 - accuracy: 0.4890 - val_loss: 1.3290 - val_accuracy: 0.4786\n",
      "Epoch 22/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.3254 - accuracy: 0.4804 - val_loss: 1.3142 - val_accuracy: 0.4901\n",
      "Epoch 23/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.3266 - accuracy: 0.4872 - val_loss: 1.3152 - val_accuracy: 0.4901\n",
      "Epoch 24/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.3140 - accuracy: 0.4888 - val_loss: 1.3241 - val_accuracy: 0.4867\n",
      "Epoch 25/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.3249 - accuracy: 0.4860 - val_loss: 1.3368 - val_accuracy: 0.4812\n",
      "Epoch 26/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.3135 - accuracy: 0.4869 - val_loss: 1.3263 - val_accuracy: 0.4947\n",
      "Epoch 27/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.3184 - accuracy: 0.4899 - val_loss: 1.3236 - val_accuracy: 0.4824\n",
      "Epoch 28/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.3260 - accuracy: 0.4903 - val_loss: 1.3364 - val_accuracy: 0.4795\n",
      "Epoch 29/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.3175 - accuracy: 0.4910 - val_loss: 1.3097 - val_accuracy: 0.4922\n",
      "Epoch 30/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.3154 - accuracy: 0.4899 - val_loss: 1.3171 - val_accuracy: 0.4968\n",
      "Epoch 31/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.3129 - accuracy: 0.4897 - val_loss: 1.3276 - val_accuracy: 0.4765\n",
      "Epoch 32/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.3227 - accuracy: 0.4888 - val_loss: 1.3203 - val_accuracy: 0.4837\n",
      "Epoch 33/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.3228 - accuracy: 0.4865 - val_loss: 1.3143 - val_accuracy: 0.4926\n",
      "Epoch 34/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.3049 - accuracy: 0.4927 - val_loss: 1.3210 - val_accuracy: 0.4905\n",
      "Epoch 35/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.3103 - accuracy: 0.4920 - val_loss: 1.3219 - val_accuracy: 0.4820\n",
      "Epoch 36/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.3075 - accuracy: 0.4935 - val_loss: 1.3152 - val_accuracy: 0.4879\n",
      "Epoch 37/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.3069 - accuracy: 0.4924 - val_loss: 1.3067 - val_accuracy: 0.4917\n",
      "Epoch 38/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.3069 - accuracy: 0.4924 - val_loss: 1.3257 - val_accuracy: 0.4913\n",
      "Epoch 39/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2957 - accuracy: 0.5022 - val_loss: 1.3242 - val_accuracy: 0.4820\n",
      "Epoch 40/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.2985 - accuracy: 0.4954 - val_loss: 1.3258 - val_accuracy: 0.4765\n",
      "Epoch 41/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.3097 - accuracy: 0.4934 - val_loss: 1.3196 - val_accuracy: 0.4858\n",
      "Epoch 42/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.3016 - accuracy: 0.4997 - val_loss: 1.3268 - val_accuracy: 0.4829\n",
      "Epoch 43/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2980 - accuracy: 0.4939 - val_loss: 1.3217 - val_accuracy: 0.4968\n",
      "Epoch 44/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2970 - accuracy: 0.4987 - val_loss: 1.3208 - val_accuracy: 0.4833\n",
      "Epoch 45/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.3008 - accuracy: 0.4966 - val_loss: 1.3204 - val_accuracy: 0.4913\n",
      "Epoch 46/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.3077 - accuracy: 0.4959 - val_loss: 1.2993 - val_accuracy: 0.5028\n",
      "Epoch 47/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.3032 - accuracy: 0.4964 - val_loss: 1.3150 - val_accuracy: 0.5040\n",
      "Epoch 48/1000\n",
      "296/296 [==============================] - 0s 706us/step - loss: 1.2976 - accuracy: 0.4985 - val_loss: 1.3088 - val_accuracy: 0.4968\n",
      "Epoch 49/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2982 - accuracy: 0.4963 - val_loss: 1.3219 - val_accuracy: 0.4807\n",
      "Epoch 50/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2888 - accuracy: 0.4993 - val_loss: 1.3114 - val_accuracy: 0.4854\n",
      "Epoch 51/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2970 - accuracy: 0.4990 - val_loss: 1.3233 - val_accuracy: 0.4913\n",
      "Epoch 52/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.2879 - accuracy: 0.5004 - val_loss: 1.3140 - val_accuracy: 0.5006\n",
      "Epoch 53/1000\n",
      "296/296 [==============================] - 0s 709us/step - loss: 1.2915 - accuracy: 0.4981 - val_loss: 1.3259 - val_accuracy: 0.4837\n",
      "Epoch 54/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2940 - accuracy: 0.4990 - val_loss: 1.3287 - val_accuracy: 0.4901\n",
      "Epoch 55/1000\n",
      "296/296 [==============================] - 0s 708us/step - loss: 1.3004 - accuracy: 0.5022 - val_loss: 1.3148 - val_accuracy: 0.4896\n",
      "Epoch 56/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.3117 - accuracy: 0.4986 - val_loss: 1.3224 - val_accuracy: 0.4816\n",
      "Epoch 57/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2896 - accuracy: 0.5003 - val_loss: 1.3117 - val_accuracy: 0.4909\n",
      "Epoch 58/1000\n",
      "296/296 [==============================] - 0s 703us/step - loss: 1.2896 - accuracy: 0.5000 - val_loss: 1.3233 - val_accuracy: 0.4850\n",
      "Epoch 59/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2962 - accuracy: 0.4975 - val_loss: 1.3184 - val_accuracy: 0.4884\n",
      "Epoch 60/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2904 - accuracy: 0.4993 - val_loss: 1.3420 - val_accuracy: 0.4913\n",
      "Epoch 61/1000\n",
      "296/296 [==============================] - 0s 707us/step - loss: 1.2993 - accuracy: 0.5037 - val_loss: 1.3129 - val_accuracy: 0.4892\n",
      "Epoch 62/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2853 - accuracy: 0.5015 - val_loss: 1.3138 - val_accuracy: 0.4879\n",
      "Epoch 63/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2920 - accuracy: 0.4996 - val_loss: 1.3451 - val_accuracy: 0.4909\n",
      "Epoch 64/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2909 - accuracy: 0.4994 - val_loss: 1.3278 - val_accuracy: 0.4879\n",
      "Epoch 65/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.2856 - accuracy: 0.5024 - val_loss: 1.3491 - val_accuracy: 0.4791\n",
      "Epoch 66/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2866 - accuracy: 0.5031 - val_loss: 1.3348 - val_accuracy: 0.4824\n",
      "Epoch 67/1000\n",
      "296/296 [==============================] - 0s 705us/step - loss: 1.2866 - accuracy: 0.5034 - val_loss: 1.3389 - val_accuracy: 0.4795\n",
      "Epoch 68/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2874 - accuracy: 0.5036 - val_loss: 1.3375 - val_accuracy: 0.4791\n",
      "Epoch 69/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2780 - accuracy: 0.5006 - val_loss: 1.3197 - val_accuracy: 0.4879\n",
      "Epoch 70/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2806 - accuracy: 0.5024 - val_loss: 1.3226 - val_accuracy: 0.4896\n",
      "Epoch 71/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2781 - accuracy: 0.5050 - val_loss: 1.3200 - val_accuracy: 0.4841\n",
      "Epoch 72/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2764 - accuracy: 0.4985 - val_loss: 1.3163 - val_accuracy: 0.4960\n",
      "Epoch 73/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.2848 - accuracy: 0.5038 - val_loss: 1.3372 - val_accuracy: 0.4706\n",
      "Epoch 74/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2743 - accuracy: 0.5116 - val_loss: 1.3157 - val_accuracy: 0.4896\n",
      "Epoch 75/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2819 - accuracy: 0.4988 - val_loss: 1.3196 - val_accuracy: 0.4922\n",
      "Epoch 76/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2788 - accuracy: 0.5012 - val_loss: 1.3422 - val_accuracy: 0.4761\n",
      "Epoch 77/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2702 - accuracy: 0.5036 - val_loss: 1.3388 - val_accuracy: 0.4774\n",
      "Epoch 78/1000\n",
      "296/296 [==============================] - 0s 706us/step - loss: 1.2898 - accuracy: 0.5068 - val_loss: 1.3340 - val_accuracy: 0.4892\n",
      "Epoch 79/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2816 - accuracy: 0.5085 - val_loss: 1.3184 - val_accuracy: 0.4922\n",
      "Epoch 80/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2769 - accuracy: 0.5069 - val_loss: 1.3287 - val_accuracy: 0.4858\n",
      "Epoch 81/1000\n",
      "296/296 [==============================] - 0s 707us/step - loss: 1.2748 - accuracy: 0.5077 - val_loss: 1.3258 - val_accuracy: 0.4905\n",
      "Epoch 82/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.2717 - accuracy: 0.5119 - val_loss: 1.3198 - val_accuracy: 0.4892\n",
      "Epoch 83/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2755 - accuracy: 0.5038 - val_loss: 1.3164 - val_accuracy: 0.4913\n",
      "Epoch 84/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2734 - accuracy: 0.5047 - val_loss: 1.3241 - val_accuracy: 0.4947\n",
      "Epoch 85/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2739 - accuracy: 0.5066 - val_loss: 1.3347 - val_accuracy: 0.4846\n",
      "Epoch 86/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2752 - accuracy: 0.5057 - val_loss: 1.3157 - val_accuracy: 0.4816\n",
      "Epoch 87/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.2685 - accuracy: 0.5068 - val_loss: 1.3340 - val_accuracy: 0.4829\n",
      "Epoch 88/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2801 - accuracy: 0.5108 - val_loss: 1.3150 - val_accuracy: 0.4888\n",
      "Epoch 89/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2684 - accuracy: 0.5093 - val_loss: 1.3182 - val_accuracy: 0.4833\n",
      "Epoch 90/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2673 - accuracy: 0.5021 - val_loss: 1.3322 - val_accuracy: 0.4841\n",
      "Epoch 91/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2632 - accuracy: 0.5113 - val_loss: 1.3401 - val_accuracy: 0.4782\n",
      "Epoch 92/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2630 - accuracy: 0.5130 - val_loss: 1.3361 - val_accuracy: 0.4765\n",
      "Epoch 93/1000\n",
      "296/296 [==============================] - 0s 708us/step - loss: 1.2651 - accuracy: 0.5114 - val_loss: 1.3300 - val_accuracy: 0.4913\n",
      "Epoch 94/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2764 - accuracy: 0.5105 - val_loss: 1.3160 - val_accuracy: 0.4968\n",
      "Epoch 95/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2685 - accuracy: 0.5086 - val_loss: 1.3239 - val_accuracy: 0.4934\n",
      "Epoch 96/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2686 - accuracy: 0.5102 - val_loss: 1.3174 - val_accuracy: 0.4871\n",
      "Epoch 97/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2689 - accuracy: 0.5057 - val_loss: 1.3346 - val_accuracy: 0.4786\n",
      "Epoch 98/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2852 - accuracy: 0.5058 - val_loss: 1.3297 - val_accuracy: 0.4850\n",
      "Epoch 99/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2747 - accuracy: 0.5028 - val_loss: 1.3246 - val_accuracy: 0.4803\n",
      "Epoch 100/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2651 - accuracy: 0.5044 - val_loss: 1.3124 - val_accuracy: 0.4968\n",
      "Epoch 101/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.2707 - accuracy: 0.5102 - val_loss: 1.3168 - val_accuracy: 0.4951\n",
      "Epoch 102/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2645 - accuracy: 0.5180 - val_loss: 1.3156 - val_accuracy: 0.4926\n",
      "Epoch 103/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2667 - accuracy: 0.5087 - val_loss: 1.3301 - val_accuracy: 0.4799\n",
      "Epoch 104/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.2673 - accuracy: 0.5095 - val_loss: 1.3276 - val_accuracy: 0.4917\n",
      "Epoch 105/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2575 - accuracy: 0.5154 - val_loss: 1.3202 - val_accuracy: 0.4913\n",
      "Epoch 106/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2552 - accuracy: 0.5140 - val_loss: 1.3239 - val_accuracy: 0.4829\n",
      "Epoch 107/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2644 - accuracy: 0.5086 - val_loss: 1.3298 - val_accuracy: 0.4812\n",
      "Epoch 108/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2669 - accuracy: 0.5128 - val_loss: 1.3360 - val_accuracy: 0.4837\n",
      "Epoch 109/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2502 - accuracy: 0.5178 - val_loss: 1.3306 - val_accuracy: 0.4820\n",
      "Epoch 110/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2645 - accuracy: 0.5093 - val_loss: 1.3419 - val_accuracy: 0.4888\n",
      "Epoch 111/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2594 - accuracy: 0.5062 - val_loss: 1.3177 - val_accuracy: 0.4884\n",
      "Epoch 112/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2530 - accuracy: 0.5144 - val_loss: 1.3354 - val_accuracy: 0.4816\n",
      "Epoch 113/1000\n",
      "296/296 [==============================] - 0s 706us/step - loss: 1.2749 - accuracy: 0.5086 - val_loss: 1.3322 - val_accuracy: 0.4922\n",
      "Epoch 114/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2629 - accuracy: 0.5146 - val_loss: 1.3331 - val_accuracy: 0.4774\n",
      "Epoch 115/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.2704 - accuracy: 0.5076 - val_loss: 1.3198 - val_accuracy: 0.4905\n",
      "Epoch 116/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2483 - accuracy: 0.5205 - val_loss: 1.3518 - val_accuracy: 0.4829\n",
      "Epoch 117/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2478 - accuracy: 0.5160 - val_loss: 1.3432 - val_accuracy: 0.4774\n",
      "Epoch 118/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2584 - accuracy: 0.5139 - val_loss: 1.3315 - val_accuracy: 0.4807\n",
      "Epoch 119/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.2498 - accuracy: 0.5201 - val_loss: 1.3185 - val_accuracy: 0.4829\n",
      "Epoch 120/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.2577 - accuracy: 0.5142 - val_loss: 1.3355 - val_accuracy: 0.4833\n",
      "Epoch 121/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2575 - accuracy: 0.5145 - val_loss: 1.3423 - val_accuracy: 0.4905\n",
      "Epoch 122/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2663 - accuracy: 0.5116 - val_loss: 1.3288 - val_accuracy: 0.4884\n",
      "Epoch 123/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2578 - accuracy: 0.5160 - val_loss: 1.3372 - val_accuracy: 0.4740\n",
      "Epoch 124/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2540 - accuracy: 0.5122 - val_loss: 1.3299 - val_accuracy: 0.4858\n",
      "Epoch 125/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2426 - accuracy: 0.5184 - val_loss: 1.3227 - val_accuracy: 0.4862\n",
      "Epoch 126/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2489 - accuracy: 0.5161 - val_loss: 1.3438 - val_accuracy: 0.4862\n",
      "Epoch 127/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2483 - accuracy: 0.5175 - val_loss: 1.3305 - val_accuracy: 0.4896\n",
      "Epoch 128/1000\n",
      "296/296 [==============================] - 0s 709us/step - loss: 1.2501 - accuracy: 0.5224 - val_loss: 1.3271 - val_accuracy: 0.4939\n",
      "Epoch 129/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.2464 - accuracy: 0.5169 - val_loss: 1.3228 - val_accuracy: 0.4854\n",
      "Epoch 130/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2511 - accuracy: 0.5119 - val_loss: 1.3300 - val_accuracy: 0.4947\n",
      "Epoch 131/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2553 - accuracy: 0.5107 - val_loss: 1.3202 - val_accuracy: 0.5032\n",
      "Epoch 132/1000\n",
      "296/296 [==============================] - 0s 715us/step - loss: 1.2571 - accuracy: 0.5160 - val_loss: 1.3342 - val_accuracy: 0.4867\n",
      "Epoch 133/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2561 - accuracy: 0.5187 - val_loss: 1.3370 - val_accuracy: 0.4951\n",
      "Epoch 134/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2588 - accuracy: 0.5144 - val_loss: 1.3339 - val_accuracy: 0.4888\n",
      "Epoch 135/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.2465 - accuracy: 0.5114 - val_loss: 1.3374 - val_accuracy: 0.4926\n",
      "Epoch 136/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2504 - accuracy: 0.5144 - val_loss: 1.3396 - val_accuracy: 0.4816\n",
      "Epoch 137/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2487 - accuracy: 0.5127 - val_loss: 1.3479 - val_accuracy: 0.4812\n",
      "Epoch 138/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2593 - accuracy: 0.5101 - val_loss: 1.3266 - val_accuracy: 0.4812\n",
      "Epoch 139/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2485 - accuracy: 0.5213 - val_loss: 1.3564 - val_accuracy: 0.4858\n",
      "Epoch 140/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2572 - accuracy: 0.5161 - val_loss: 1.3266 - val_accuracy: 0.4816\n",
      "Epoch 141/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2457 - accuracy: 0.5152 - val_loss: 1.3440 - val_accuracy: 0.4807\n",
      "Epoch 142/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2437 - accuracy: 0.5165 - val_loss: 1.3275 - val_accuracy: 0.4854\n",
      "Epoch 143/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2439 - accuracy: 0.5188 - val_loss: 1.3354 - val_accuracy: 0.4871\n",
      "Epoch 144/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2440 - accuracy: 0.5192 - val_loss: 1.3469 - val_accuracy: 0.4714\n",
      "Epoch 145/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2506 - accuracy: 0.5131 - val_loss: 1.3489 - val_accuracy: 0.4774\n",
      "Epoch 146/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2443 - accuracy: 0.5172 - val_loss: 1.3470 - val_accuracy: 0.4922\n",
      "Epoch 147/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2388 - accuracy: 0.5194 - val_loss: 1.3368 - val_accuracy: 0.4791\n",
      "Epoch 148/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2482 - accuracy: 0.5211 - val_loss: 1.3293 - val_accuracy: 0.4833\n",
      "Epoch 149/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2414 - accuracy: 0.5178 - val_loss: 1.3398 - val_accuracy: 0.4867\n",
      "Epoch 150/1000\n",
      "296/296 [==============================] - 0s 711us/step - loss: 1.2505 - accuracy: 0.5199 - val_loss: 1.3488 - val_accuracy: 0.4748\n",
      "Epoch 151/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2551 - accuracy: 0.5158 - val_loss: 1.3338 - val_accuracy: 0.4951\n",
      "Epoch 152/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2481 - accuracy: 0.5239 - val_loss: 1.3452 - val_accuracy: 0.4791\n",
      "Epoch 153/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2529 - accuracy: 0.5190 - val_loss: 1.3260 - val_accuracy: 0.4972\n",
      "Epoch 154/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2409 - accuracy: 0.5168 - val_loss: 1.3357 - val_accuracy: 0.4803\n",
      "Epoch 155/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2454 - accuracy: 0.5223 - val_loss: 1.3467 - val_accuracy: 0.4901\n",
      "Epoch 156/1000\n",
      "296/296 [==============================] - 0s 710us/step - loss: 1.2531 - accuracy: 0.5203 - val_loss: 1.3351 - val_accuracy: 0.4765\n",
      "Epoch 157/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2559 - accuracy: 0.5139 - val_loss: 1.3441 - val_accuracy: 0.4846\n",
      "Epoch 158/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2601 - accuracy: 0.5161 - val_loss: 1.3366 - val_accuracy: 0.4956\n",
      "Epoch 159/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.2445 - accuracy: 0.5228 - val_loss: 1.3413 - val_accuracy: 0.4833\n",
      "Epoch 160/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.2421 - accuracy: 0.5198 - val_loss: 1.3341 - val_accuracy: 0.4956\n",
      "Epoch 161/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2561 - accuracy: 0.5204 - val_loss: 1.3439 - val_accuracy: 0.4909\n",
      "Epoch 162/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.2431 - accuracy: 0.5215 - val_loss: 1.3574 - val_accuracy: 0.4812\n",
      "Epoch 163/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.2429 - accuracy: 0.5252 - val_loss: 1.3521 - val_accuracy: 0.4774\n",
      "Epoch 164/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2367 - accuracy: 0.5201 - val_loss: 1.3502 - val_accuracy: 0.4795\n",
      "Epoch 165/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.2530 - accuracy: 0.5160 - val_loss: 1.3474 - val_accuracy: 0.4854\n",
      "Epoch 166/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.2381 - accuracy: 0.5216 - val_loss: 1.3384 - val_accuracy: 0.4875\n",
      "Epoch 167/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.2469 - accuracy: 0.5200 - val_loss: 1.3445 - val_accuracy: 0.4812\n",
      "Epoch 168/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.2394 - accuracy: 0.5189 - val_loss: 1.3440 - val_accuracy: 0.4930\n",
      "Epoch 169/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2390 - accuracy: 0.5241 - val_loss: 1.3347 - val_accuracy: 0.4854\n",
      "Epoch 170/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.2391 - accuracy: 0.5194 - val_loss: 1.3429 - val_accuracy: 0.4892\n",
      "Epoch 171/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2388 - accuracy: 0.5223 - val_loss: 1.3339 - val_accuracy: 0.4875\n",
      "Epoch 172/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2321 - accuracy: 0.5235 - val_loss: 1.3555 - val_accuracy: 0.4744\n",
      "Epoch 173/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2483 - accuracy: 0.5198 - val_loss: 1.3403 - val_accuracy: 0.4803\n",
      "Epoch 174/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2371 - accuracy: 0.5152 - val_loss: 1.3406 - val_accuracy: 0.4875\n",
      "Epoch 175/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2413 - accuracy: 0.5238 - val_loss: 1.3233 - val_accuracy: 0.4896\n",
      "Epoch 176/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2486 - accuracy: 0.5194 - val_loss: 1.3400 - val_accuracy: 0.4905\n",
      "Epoch 177/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2440 - accuracy: 0.5150 - val_loss: 1.3492 - val_accuracy: 0.4879\n",
      "Epoch 178/1000\n",
      "296/296 [==============================] - 0s 714us/step - loss: 1.2274 - accuracy: 0.5234 - val_loss: 1.3419 - val_accuracy: 0.4833\n",
      "Epoch 179/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2306 - accuracy: 0.5235 - val_loss: 1.3312 - val_accuracy: 0.4964\n",
      "Epoch 180/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2448 - accuracy: 0.5268 - val_loss: 1.3395 - val_accuracy: 0.4820\n",
      "Epoch 181/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.2474 - accuracy: 0.5157 - val_loss: 1.3335 - val_accuracy: 0.4896\n",
      "Epoch 182/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2409 - accuracy: 0.5205 - val_loss: 1.3394 - val_accuracy: 0.4761\n",
      "Epoch 183/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2305 - accuracy: 0.5220 - val_loss: 1.3418 - val_accuracy: 0.4799\n",
      "Epoch 184/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2457 - accuracy: 0.5182 - val_loss: 1.3371 - val_accuracy: 0.4930\n",
      "Epoch 185/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2417 - accuracy: 0.5192 - val_loss: 1.3532 - val_accuracy: 0.4824\n",
      "Epoch 186/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.2468 - accuracy: 0.5222 - val_loss: 1.3343 - val_accuracy: 0.4867\n",
      "Epoch 187/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.2308 - accuracy: 0.5231 - val_loss: 1.3422 - val_accuracy: 0.4871\n",
      "Epoch 188/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2488 - accuracy: 0.5187 - val_loss: 1.3303 - val_accuracy: 0.4884\n",
      "Epoch 189/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2301 - accuracy: 0.5263 - val_loss: 1.3598 - val_accuracy: 0.4926\n",
      "Epoch 190/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2314 - accuracy: 0.5216 - val_loss: 1.3459 - val_accuracy: 0.4786\n",
      "Epoch 191/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2316 - accuracy: 0.5249 - val_loss: 1.3298 - val_accuracy: 0.4833\n",
      "Epoch 192/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2370 - accuracy: 0.5207 - val_loss: 1.3439 - val_accuracy: 0.4829\n",
      "Epoch 193/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2353 - accuracy: 0.5183 - val_loss: 1.3472 - val_accuracy: 0.4795\n",
      "Epoch 194/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2431 - accuracy: 0.5215 - val_loss: 1.3461 - val_accuracy: 0.4862\n",
      "Epoch 195/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2377 - accuracy: 0.5193 - val_loss: 1.3543 - val_accuracy: 0.4846\n",
      "Epoch 196/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2369 - accuracy: 0.5230 - val_loss: 1.3522 - val_accuracy: 0.4862\n",
      "Epoch 197/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2407 - accuracy: 0.5218 - val_loss: 1.3414 - val_accuracy: 0.4799\n",
      "Epoch 198/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2401 - accuracy: 0.5204 - val_loss: 1.3414 - val_accuracy: 0.4824\n",
      "Epoch 199/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2268 - accuracy: 0.5218 - val_loss: 1.3570 - val_accuracy: 0.4858\n",
      "Epoch 200/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2340 - accuracy: 0.5269 - val_loss: 1.3408 - val_accuracy: 0.4892\n",
      "Epoch 201/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2287 - accuracy: 0.5218 - val_loss: 1.3472 - val_accuracy: 0.4786\n",
      "Epoch 202/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2293 - accuracy: 0.5255 - val_loss: 1.3670 - val_accuracy: 0.4719\n",
      "Epoch 203/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2386 - accuracy: 0.5200 - val_loss: 1.3537 - val_accuracy: 0.4875\n",
      "Epoch 204/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2227 - accuracy: 0.5248 - val_loss: 1.3429 - val_accuracy: 0.4858\n",
      "Epoch 205/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2311 - accuracy: 0.5204 - val_loss: 1.3326 - val_accuracy: 0.4964\n",
      "Epoch 206/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2342 - accuracy: 0.5288 - val_loss: 1.3507 - val_accuracy: 0.4862\n",
      "Epoch 207/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2451 - accuracy: 0.5221 - val_loss: 1.3385 - val_accuracy: 0.4896\n",
      "Epoch 208/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.2410 - accuracy: 0.5235 - val_loss: 1.3554 - val_accuracy: 0.4934\n",
      "Epoch 209/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2345 - accuracy: 0.5278 - val_loss: 1.3536 - val_accuracy: 0.4829\n",
      "Epoch 210/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2303 - accuracy: 0.5270 - val_loss: 1.3356 - val_accuracy: 0.4972\n",
      "Epoch 211/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2265 - accuracy: 0.5226 - val_loss: 1.3603 - val_accuracy: 0.4871\n",
      "Epoch 212/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2429 - accuracy: 0.5220 - val_loss: 1.3532 - val_accuracy: 0.4956\n",
      "Epoch 213/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2252 - accuracy: 0.5277 - val_loss: 1.3514 - val_accuracy: 0.4943\n",
      "Epoch 214/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2258 - accuracy: 0.5237 - val_loss: 1.3590 - val_accuracy: 0.4879\n",
      "Epoch 215/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2232 - accuracy: 0.5226 - val_loss: 1.3512 - val_accuracy: 0.4846\n",
      "Epoch 216/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2273 - accuracy: 0.5239 - val_loss: 1.3369 - val_accuracy: 0.4884\n",
      "Epoch 217/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2272 - accuracy: 0.5274 - val_loss: 1.3451 - val_accuracy: 0.4964\n",
      "Epoch 218/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2278 - accuracy: 0.5291 - val_loss: 1.3536 - val_accuracy: 0.4905\n",
      "Epoch 219/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2396 - accuracy: 0.5229 - val_loss: 1.3491 - val_accuracy: 0.4858\n",
      "Epoch 220/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2361 - accuracy: 0.5279 - val_loss: 1.3422 - val_accuracy: 0.4858\n",
      "Epoch 221/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2234 - accuracy: 0.5302 - val_loss: 1.3554 - val_accuracy: 0.4862\n",
      "Epoch 222/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2264 - accuracy: 0.5229 - val_loss: 1.3616 - val_accuracy: 0.4769\n",
      "Epoch 223/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2282 - accuracy: 0.5237 - val_loss: 1.3641 - val_accuracy: 0.4858\n",
      "Epoch 224/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2202 - accuracy: 0.5242 - val_loss: 1.3401 - val_accuracy: 0.4943\n",
      "Epoch 225/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2306 - accuracy: 0.5258 - val_loss: 1.3458 - val_accuracy: 0.4778\n",
      "Epoch 226/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2357 - accuracy: 0.5216 - val_loss: 1.3428 - val_accuracy: 0.4956\n",
      "Epoch 227/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2291 - accuracy: 0.5304 - val_loss: 1.3476 - val_accuracy: 0.4871\n",
      "Epoch 228/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2214 - accuracy: 0.5266 - val_loss: 1.3402 - val_accuracy: 0.4892\n",
      "Epoch 229/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2334 - accuracy: 0.5289 - val_loss: 1.3426 - val_accuracy: 0.4913\n",
      "Epoch 230/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2432 - accuracy: 0.5200 - val_loss: 1.3585 - val_accuracy: 0.4884\n",
      "Epoch 231/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2383 - accuracy: 0.5241 - val_loss: 1.3724 - val_accuracy: 0.4782\n",
      "Epoch 232/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2216 - accuracy: 0.5251 - val_loss: 1.3559 - val_accuracy: 0.4846\n",
      "Epoch 233/1000\n",
      "296/296 [==============================] - 0s 709us/step - loss: 1.2256 - accuracy: 0.5320 - val_loss: 1.3643 - val_accuracy: 0.4850\n",
      "Epoch 234/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2378 - accuracy: 0.5302 - val_loss: 1.3557 - val_accuracy: 0.4761\n",
      "Epoch 235/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2273 - accuracy: 0.5258 - val_loss: 1.3313 - val_accuracy: 0.4977\n",
      "Epoch 236/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2276 - accuracy: 0.5236 - val_loss: 1.3509 - val_accuracy: 0.4879\n",
      "Epoch 237/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.2238 - accuracy: 0.5259 - val_loss: 1.3523 - val_accuracy: 0.4829\n",
      "Epoch 238/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2218 - accuracy: 0.5225 - val_loss: 1.3398 - val_accuracy: 0.4841\n",
      "Epoch 239/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2170 - accuracy: 0.5317 - val_loss: 1.3325 - val_accuracy: 0.4884\n",
      "Epoch 240/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2198 - accuracy: 0.5197 - val_loss: 1.3547 - val_accuracy: 0.4854\n",
      "Epoch 241/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.2290 - accuracy: 0.5263 - val_loss: 1.3463 - val_accuracy: 0.4917\n",
      "Epoch 242/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2322 - accuracy: 0.5256 - val_loss: 1.3604 - val_accuracy: 0.4862\n",
      "Epoch 243/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2204 - accuracy: 0.5285 - val_loss: 1.3518 - val_accuracy: 0.4854\n",
      "Epoch 244/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2314 - accuracy: 0.5238 - val_loss: 1.3592 - val_accuracy: 0.4769\n",
      "Epoch 245/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2255 - accuracy: 0.5343 - val_loss: 1.3428 - val_accuracy: 0.4803\n",
      "Epoch 246/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2232 - accuracy: 0.5308 - val_loss: 1.3562 - val_accuracy: 0.4748\n",
      "Epoch 247/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2260 - accuracy: 0.5275 - val_loss: 1.3629 - val_accuracy: 0.4791\n",
      "Epoch 248/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2235 - accuracy: 0.5302 - val_loss: 1.3564 - val_accuracy: 0.4862\n",
      "Epoch 249/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2280 - accuracy: 0.5281 - val_loss: 1.3813 - val_accuracy: 0.4799\n",
      "Epoch 250/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2261 - accuracy: 0.5279 - val_loss: 1.3667 - val_accuracy: 0.4714\n",
      "Epoch 251/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.2318 - accuracy: 0.5273 - val_loss: 1.3594 - val_accuracy: 0.4807\n",
      "Epoch 252/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2116 - accuracy: 0.5284 - val_loss: 1.3720 - val_accuracy: 0.4761\n",
      "Epoch 253/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2291 - accuracy: 0.5272 - val_loss: 1.3598 - val_accuracy: 0.4799\n",
      "Epoch 254/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2200 - accuracy: 0.5339 - val_loss: 1.3715 - val_accuracy: 0.4761\n",
      "Epoch 255/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.2144 - accuracy: 0.5338 - val_loss: 1.3498 - val_accuracy: 0.4888\n",
      "Epoch 256/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2159 - accuracy: 0.5389 - val_loss: 1.3622 - val_accuracy: 0.4812\n",
      "Epoch 257/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2146 - accuracy: 0.5288 - val_loss: 1.3658 - val_accuracy: 0.4812\n",
      "Epoch 258/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.2208 - accuracy: 0.5308 - val_loss: 1.3670 - val_accuracy: 0.4867\n",
      "Epoch 259/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.2281 - accuracy: 0.5275 - val_loss: 1.3583 - val_accuracy: 0.4901\n",
      "Epoch 260/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.2416 - accuracy: 0.5198 - val_loss: 1.3720 - val_accuracy: 0.4761\n",
      "Epoch 261/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2306 - accuracy: 0.5281 - val_loss: 1.3749 - val_accuracy: 0.4638\n",
      "Epoch 262/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2188 - accuracy: 0.5299 - val_loss: 1.3507 - val_accuracy: 0.4807\n",
      "Epoch 263/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2293 - accuracy: 0.5286 - val_loss: 1.3594 - val_accuracy: 0.4799\n",
      "Epoch 264/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2246 - accuracy: 0.5285 - val_loss: 1.3586 - val_accuracy: 0.4854\n",
      "Epoch 265/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2163 - accuracy: 0.5319 - val_loss: 1.3656 - val_accuracy: 0.4736\n",
      "Epoch 266/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2047 - accuracy: 0.5314 - val_loss: 1.3639 - val_accuracy: 0.4820\n",
      "Epoch 267/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2238 - accuracy: 0.5306 - val_loss: 1.3621 - val_accuracy: 0.4744\n",
      "Epoch 268/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2259 - accuracy: 0.5269 - val_loss: 1.3416 - val_accuracy: 0.4850\n",
      "Epoch 269/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.2263 - accuracy: 0.5292 - val_loss: 1.3517 - val_accuracy: 0.4829\n",
      "Epoch 270/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2268 - accuracy: 0.5280 - val_loss: 1.3466 - val_accuracy: 0.4765\n",
      "Epoch 271/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2251 - accuracy: 0.5248 - val_loss: 1.3494 - val_accuracy: 0.4858\n",
      "Epoch 272/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.2170 - accuracy: 0.5281 - val_loss: 1.3676 - val_accuracy: 0.4816\n",
      "Epoch 273/1000\n",
      "296/296 [==============================] - 0s 713us/step - loss: 1.2203 - accuracy: 0.5333 - val_loss: 1.3595 - val_accuracy: 0.4824\n",
      "Epoch 274/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2145 - accuracy: 0.5338 - val_loss: 1.3487 - val_accuracy: 0.4854\n",
      "Epoch 275/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2161 - accuracy: 0.5293 - val_loss: 1.3553 - val_accuracy: 0.4901\n",
      "Epoch 276/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2213 - accuracy: 0.5276 - val_loss: 1.3731 - val_accuracy: 0.4934\n",
      "Epoch 277/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2242 - accuracy: 0.5290 - val_loss: 1.3629 - val_accuracy: 0.4909\n",
      "Epoch 278/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.2133 - accuracy: 0.5303 - val_loss: 1.3443 - val_accuracy: 0.4778\n",
      "Epoch 279/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2248 - accuracy: 0.5293 - val_loss: 1.3377 - val_accuracy: 0.4837\n",
      "Epoch 280/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2068 - accuracy: 0.5347 - val_loss: 1.3583 - val_accuracy: 0.4854\n",
      "Epoch 281/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2116 - accuracy: 0.5323 - val_loss: 1.3444 - val_accuracy: 0.4820\n",
      "Epoch 282/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2169 - accuracy: 0.5360 - val_loss: 1.3411 - val_accuracy: 0.4850\n",
      "Epoch 283/1000\n",
      "296/296 [==============================] - 0s 712us/step - loss: 1.2199 - accuracy: 0.5297 - val_loss: 1.3536 - val_accuracy: 0.4736\n",
      "Epoch 284/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.2104 - accuracy: 0.5372 - val_loss: 1.3648 - val_accuracy: 0.4829\n",
      "Epoch 285/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.2280 - accuracy: 0.5271 - val_loss: 1.3461 - val_accuracy: 0.4867\n",
      "Epoch 286/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2166 - accuracy: 0.5309 - val_loss: 1.3421 - val_accuracy: 0.4850\n",
      "Epoch 287/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2189 - accuracy: 0.5368 - val_loss: 1.3732 - val_accuracy: 0.4795\n",
      "Epoch 288/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2102 - accuracy: 0.5404 - val_loss: 1.3450 - val_accuracy: 0.4892\n",
      "Epoch 289/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2114 - accuracy: 0.5360 - val_loss: 1.3652 - val_accuracy: 0.4799\n",
      "Epoch 290/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2167 - accuracy: 0.5312 - val_loss: 1.3504 - val_accuracy: 0.4858\n",
      "Epoch 291/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.2289 - accuracy: 0.5290 - val_loss: 1.3455 - val_accuracy: 0.4879\n",
      "Epoch 292/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.2286 - accuracy: 0.5304 - val_loss: 1.3529 - val_accuracy: 0.4786\n",
      "Epoch 293/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.2229 - accuracy: 0.5291 - val_loss: 1.3578 - val_accuracy: 0.4871\n",
      "Epoch 294/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.2068 - accuracy: 0.5374 - val_loss: 1.3462 - val_accuracy: 0.4807\n",
      "Epoch 295/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.2152 - accuracy: 0.5307 - val_loss: 1.3556 - val_accuracy: 0.4837\n",
      "Epoch 296/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2371 - accuracy: 0.5301 - val_loss: 1.3506 - val_accuracy: 0.4795\n",
      "Epoch 297/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.2120 - accuracy: 0.5370 - val_loss: 1.3579 - val_accuracy: 0.4774\n",
      "Epoch 298/1000\n",
      "296/296 [==============================] - 0s 778us/step - loss: 1.2126 - accuracy: 0.5375 - val_loss: 1.3633 - val_accuracy: 0.4752\n",
      "Epoch 299/1000\n",
      "296/296 [==============================] - 0s 814us/step - loss: 1.2208 - accuracy: 0.5319 - val_loss: 1.3507 - val_accuracy: 0.4871\n",
      "Epoch 300/1000\n",
      "296/296 [==============================] - 0s 786us/step - loss: 1.2145 - accuracy: 0.5301 - val_loss: 1.3530 - val_accuracy: 0.4867\n",
      "Epoch 301/1000\n",
      "296/296 [==============================] - 0s 817us/step - loss: 1.2171 - accuracy: 0.5361 - val_loss: 1.3561 - val_accuracy: 0.4778\n",
      "Epoch 302/1000\n",
      "296/296 [==============================] - 0s 774us/step - loss: 1.2070 - accuracy: 0.5389 - val_loss: 1.3668 - val_accuracy: 0.4769\n",
      "Epoch 303/1000\n",
      "296/296 [==============================] - 0s 779us/step - loss: 1.2022 - accuracy: 0.5364 - val_loss: 1.3726 - val_accuracy: 0.4680\n",
      "Epoch 304/1000\n",
      "296/296 [==============================] - 0s 781us/step - loss: 1.2101 - accuracy: 0.5306 - val_loss: 1.3555 - val_accuracy: 0.4833\n",
      "Epoch 305/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.2133 - accuracy: 0.5381 - val_loss: 1.3505 - val_accuracy: 0.4892\n",
      "Epoch 306/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.2087 - accuracy: 0.5287 - val_loss: 1.3447 - val_accuracy: 0.4837\n",
      "Epoch 307/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.2444 - accuracy: 0.5233 - val_loss: 1.3447 - val_accuracy: 0.4693\n",
      "Epoch 308/1000\n",
      "296/296 [==============================] - 0s 773us/step - loss: 1.2284 - accuracy: 0.5307 - val_loss: 1.3604 - val_accuracy: 0.4888\n",
      "Epoch 309/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.2201 - accuracy: 0.5329 - val_loss: 1.3390 - val_accuracy: 0.4871\n",
      "Epoch 310/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.2104 - accuracy: 0.5375 - val_loss: 1.3525 - val_accuracy: 0.4774\n",
      "Epoch 311/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.2114 - accuracy: 0.5339 - val_loss: 1.3681 - val_accuracy: 0.4638\n",
      "Epoch 312/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2125 - accuracy: 0.5278 - val_loss: 1.3599 - val_accuracy: 0.4803\n",
      "Epoch 313/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2086 - accuracy: 0.5417 - val_loss: 1.3683 - val_accuracy: 0.4820\n",
      "Epoch 314/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.2107 - accuracy: 0.5366 - val_loss: 1.3590 - val_accuracy: 0.4947\n",
      "Epoch 315/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.2144 - accuracy: 0.5324 - val_loss: 1.3373 - val_accuracy: 0.4846\n",
      "Epoch 316/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.2047 - accuracy: 0.5365 - val_loss: 1.3684 - val_accuracy: 0.4858\n",
      "Epoch 317/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2053 - accuracy: 0.5400 - val_loss: 1.3561 - val_accuracy: 0.4820\n",
      "Epoch 318/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.2262 - accuracy: 0.5288 - val_loss: 1.3610 - val_accuracy: 0.4803\n",
      "Epoch 319/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.2028 - accuracy: 0.5377 - val_loss: 1.3548 - val_accuracy: 0.4807\n",
      "Epoch 320/1000\n",
      "296/296 [==============================] - 0s 777us/step - loss: 1.1987 - accuracy: 0.5364 - val_loss: 1.3661 - val_accuracy: 0.4829\n",
      "Epoch 321/1000\n",
      "296/296 [==============================] - 0s 814us/step - loss: 1.2059 - accuracy: 0.5363 - val_loss: 1.3619 - val_accuracy: 0.4841\n",
      "Epoch 322/1000\n",
      "296/296 [==============================] - 0s 789us/step - loss: 1.2077 - accuracy: 0.5335 - val_loss: 1.3584 - val_accuracy: 0.4858\n",
      "Epoch 323/1000\n",
      "296/296 [==============================] - 0s 790us/step - loss: 1.1984 - accuracy: 0.5360 - val_loss: 1.3662 - val_accuracy: 0.4812\n",
      "Epoch 324/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.2164 - accuracy: 0.5329 - val_loss: 1.3499 - val_accuracy: 0.4871\n",
      "Epoch 325/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.2154 - accuracy: 0.5354 - val_loss: 1.3574 - val_accuracy: 0.4879\n",
      "Epoch 326/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.2065 - accuracy: 0.5361 - val_loss: 1.3653 - val_accuracy: 0.4782\n",
      "Epoch 327/1000\n",
      "296/296 [==============================] - 0s 765us/step - loss: 1.2155 - accuracy: 0.5335 - val_loss: 1.3625 - val_accuracy: 0.4837\n",
      "Epoch 328/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.2106 - accuracy: 0.5337 - val_loss: 1.3788 - val_accuracy: 0.4807\n",
      "Epoch 329/1000\n",
      "296/296 [==============================] - 0s 771us/step - loss: 1.2089 - accuracy: 0.5354 - val_loss: 1.3760 - val_accuracy: 0.4812\n",
      "Epoch 330/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.1960 - accuracy: 0.5374 - val_loss: 1.3448 - val_accuracy: 0.4824\n",
      "Epoch 331/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2163 - accuracy: 0.5338 - val_loss: 1.3615 - val_accuracy: 0.4803\n",
      "Epoch 332/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.2012 - accuracy: 0.5394 - val_loss: 1.3745 - val_accuracy: 0.4858\n",
      "Epoch 333/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2246 - accuracy: 0.5294 - val_loss: 1.3665 - val_accuracy: 0.4744\n",
      "Epoch 334/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1920 - accuracy: 0.5424 - val_loss: 1.3717 - val_accuracy: 0.4799\n",
      "Epoch 335/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.2209 - accuracy: 0.5344 - val_loss: 1.3581 - val_accuracy: 0.4858\n",
      "Epoch 336/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2204 - accuracy: 0.5339 - val_loss: 1.3482 - val_accuracy: 0.4850\n",
      "Epoch 337/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.2108 - accuracy: 0.5338 - val_loss: 1.3572 - val_accuracy: 0.4854\n",
      "Epoch 338/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2063 - accuracy: 0.5364 - val_loss: 1.3620 - val_accuracy: 0.4820\n",
      "Epoch 339/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.2127 - accuracy: 0.5326 - val_loss: 1.3801 - val_accuracy: 0.4693\n",
      "Epoch 340/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.2114 - accuracy: 0.5390 - val_loss: 1.3644 - val_accuracy: 0.4871\n",
      "Epoch 341/1000\n",
      "296/296 [==============================] - 0s 786us/step - loss: 1.2081 - accuracy: 0.5381 - val_loss: 1.3619 - val_accuracy: 0.4829\n",
      "Epoch 342/1000\n",
      "296/296 [==============================] - 0s 797us/step - loss: 1.2184 - accuracy: 0.5363 - val_loss: 1.3722 - val_accuracy: 0.4833\n",
      "Epoch 343/1000\n",
      "296/296 [==============================] - 0s 771us/step - loss: 1.2178 - accuracy: 0.5340 - val_loss: 1.3630 - val_accuracy: 0.4879\n",
      "Epoch 344/1000\n",
      "296/296 [==============================] - 0s 816us/step - loss: 1.2166 - accuracy: 0.5319 - val_loss: 1.3577 - val_accuracy: 0.4816\n",
      "Epoch 345/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.2087 - accuracy: 0.5306 - val_loss: 1.3688 - val_accuracy: 0.4824\n",
      "Epoch 346/1000\n",
      "296/296 [==============================] - 0s 817us/step - loss: 1.2101 - accuracy: 0.5375 - val_loss: 1.3676 - val_accuracy: 0.4799\n",
      "Epoch 347/1000\n",
      "296/296 [==============================] - 0s 804us/step - loss: 1.2069 - accuracy: 0.5426 - val_loss: 1.3562 - val_accuracy: 0.4820\n",
      "Epoch 348/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.2250 - accuracy: 0.5306 - val_loss: 1.3656 - val_accuracy: 0.4862\n",
      "Epoch 349/1000\n",
      "296/296 [==============================] - 0s 787us/step - loss: 1.2065 - accuracy: 0.5346 - val_loss: 1.3598 - val_accuracy: 0.4824\n",
      "Epoch 350/1000\n",
      "296/296 [==============================] - 0s 790us/step - loss: 1.2318 - accuracy: 0.5265 - val_loss: 1.3656 - val_accuracy: 0.4744\n",
      "Epoch 351/1000\n",
      "296/296 [==============================] - 0s 817us/step - loss: 1.2020 - accuracy: 0.5356 - val_loss: 1.3717 - val_accuracy: 0.4693\n",
      "Epoch 352/1000\n",
      "296/296 [==============================] - 0s 775us/step - loss: 1.2080 - accuracy: 0.5362 - val_loss: 1.3555 - val_accuracy: 0.4951\n",
      "Epoch 353/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2188 - accuracy: 0.5325 - val_loss: 1.3428 - val_accuracy: 0.4782\n",
      "Epoch 354/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2099 - accuracy: 0.5333 - val_loss: 1.3780 - val_accuracy: 0.4816\n",
      "Epoch 355/1000\n",
      "296/296 [==============================] - 0s 754us/step - loss: 1.2186 - accuracy: 0.5361 - val_loss: 1.3634 - val_accuracy: 0.4922\n",
      "Epoch 356/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.2147 - accuracy: 0.5372 - val_loss: 1.3730 - val_accuracy: 0.4837\n",
      "Epoch 357/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2055 - accuracy: 0.5295 - val_loss: 1.3936 - val_accuracy: 0.4837\n",
      "Epoch 358/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.2047 - accuracy: 0.5357 - val_loss: 1.3704 - val_accuracy: 0.4786\n",
      "Epoch 359/1000\n",
      "296/296 [==============================] - 0s 759us/step - loss: 1.2069 - accuracy: 0.5386 - val_loss: 1.3931 - val_accuracy: 0.4761\n",
      "Epoch 360/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.2089 - accuracy: 0.5367 - val_loss: 1.3796 - val_accuracy: 0.4922\n",
      "Epoch 361/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.2053 - accuracy: 0.5381 - val_loss: 1.3617 - val_accuracy: 0.4824\n",
      "Epoch 362/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.2112 - accuracy: 0.5334 - val_loss: 1.3691 - val_accuracy: 0.4854\n",
      "Epoch 363/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.2074 - accuracy: 0.5368 - val_loss: 1.3811 - val_accuracy: 0.4799\n",
      "Epoch 364/1000\n",
      "296/296 [==============================] - 0s 764us/step - loss: 1.2065 - accuracy: 0.5326 - val_loss: 1.3538 - val_accuracy: 0.4854\n",
      "Epoch 365/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2166 - accuracy: 0.5371 - val_loss: 1.3616 - val_accuracy: 0.4888\n",
      "Epoch 366/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.2121 - accuracy: 0.5382 - val_loss: 1.3679 - val_accuracy: 0.4917\n",
      "Epoch 367/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.2116 - accuracy: 0.5297 - val_loss: 1.3773 - val_accuracy: 0.4693\n",
      "Epoch 368/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.2051 - accuracy: 0.5386 - val_loss: 1.3775 - val_accuracy: 0.4812\n",
      "Epoch 369/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2171 - accuracy: 0.5330 - val_loss: 1.3977 - val_accuracy: 0.4757\n",
      "Epoch 370/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1914 - accuracy: 0.5428 - val_loss: 1.3893 - val_accuracy: 0.4719\n",
      "Epoch 371/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.2018 - accuracy: 0.5438 - val_loss: 1.3964 - val_accuracy: 0.4761\n",
      "Epoch 372/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2000 - accuracy: 0.5381 - val_loss: 1.4100 - val_accuracy: 0.4651\n",
      "Epoch 373/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.2010 - accuracy: 0.5344 - val_loss: 1.3837 - val_accuracy: 0.4736\n",
      "Epoch 374/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.2035 - accuracy: 0.5369 - val_loss: 1.3791 - val_accuracy: 0.4862\n",
      "Epoch 375/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.2029 - accuracy: 0.5347 - val_loss: 1.3779 - val_accuracy: 0.4769\n",
      "Epoch 376/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.2009 - accuracy: 0.5426 - val_loss: 1.3713 - val_accuracy: 0.4858\n",
      "Epoch 377/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.2192 - accuracy: 0.5258 - val_loss: 1.3706 - val_accuracy: 0.4740\n",
      "Epoch 378/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.2050 - accuracy: 0.5404 - val_loss: 1.3808 - val_accuracy: 0.4846\n",
      "Epoch 379/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.2122 - accuracy: 0.5370 - val_loss: 1.3488 - val_accuracy: 0.4892\n",
      "Epoch 380/1000\n",
      "296/296 [==============================] - 0s 775us/step - loss: 1.1988 - accuracy: 0.5383 - val_loss: 1.3782 - val_accuracy: 0.4723\n",
      "Epoch 381/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2247 - accuracy: 0.5326 - val_loss: 1.3778 - val_accuracy: 0.4765\n",
      "Epoch 382/1000\n",
      "296/296 [==============================] - 0s 771us/step - loss: 1.2048 - accuracy: 0.5361 - val_loss: 1.3883 - val_accuracy: 0.4676\n",
      "Epoch 383/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.2131 - accuracy: 0.5312 - val_loss: 1.3704 - val_accuracy: 0.4799\n",
      "Epoch 384/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.2060 - accuracy: 0.5408 - val_loss: 1.3648 - val_accuracy: 0.4850\n",
      "Epoch 385/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.2211 - accuracy: 0.5326 - val_loss: 1.3603 - val_accuracy: 0.4879\n",
      "Epoch 386/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.2166 - accuracy: 0.5403 - val_loss: 1.3641 - val_accuracy: 0.4807\n",
      "Epoch 387/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.1994 - accuracy: 0.5375 - val_loss: 1.3697 - val_accuracy: 0.4850\n",
      "Epoch 388/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.2042 - accuracy: 0.5392 - val_loss: 1.3557 - val_accuracy: 0.4803\n",
      "Epoch 389/1000\n",
      "296/296 [==============================] - 0s 771us/step - loss: 1.1905 - accuracy: 0.5405 - val_loss: 1.3768 - val_accuracy: 0.4748\n",
      "Epoch 390/1000\n",
      "296/296 [==============================] - 0s 779us/step - loss: 1.1942 - accuracy: 0.5466 - val_loss: 1.3737 - val_accuracy: 0.4731\n",
      "Epoch 391/1000\n",
      "296/296 [==============================] - 0s 779us/step - loss: 1.2157 - accuracy: 0.5368 - val_loss: 1.3648 - val_accuracy: 0.4829\n",
      "Epoch 392/1000\n",
      "296/296 [==============================] - 0s 786us/step - loss: 1.1959 - accuracy: 0.5425 - val_loss: 1.3641 - val_accuracy: 0.4850\n",
      "Epoch 393/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.2102 - accuracy: 0.5312 - val_loss: 1.3514 - val_accuracy: 0.4867\n",
      "Epoch 394/1000\n",
      "296/296 [==============================] - 0s 810us/step - loss: 1.2046 - accuracy: 0.5402 - val_loss: 1.3491 - val_accuracy: 0.4812\n",
      "Epoch 395/1000\n",
      "296/296 [==============================] - 0s 784us/step - loss: 1.2120 - accuracy: 0.5392 - val_loss: 1.3660 - val_accuracy: 0.4803\n",
      "Epoch 396/1000\n",
      "296/296 [==============================] - 0s 774us/step - loss: 1.2142 - accuracy: 0.5341 - val_loss: 1.3333 - val_accuracy: 0.4837\n",
      "Epoch 397/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.2177 - accuracy: 0.5324 - val_loss: 1.3709 - val_accuracy: 0.4706\n",
      "Epoch 398/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.2061 - accuracy: 0.5396 - val_loss: 1.3545 - val_accuracy: 0.4867\n",
      "Epoch 399/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.2026 - accuracy: 0.5407 - val_loss: 1.3628 - val_accuracy: 0.4803\n",
      "Epoch 400/1000\n",
      "296/296 [==============================] - 0s 764us/step - loss: 1.2066 - accuracy: 0.5379 - val_loss: 1.3722 - val_accuracy: 0.4799\n",
      "Epoch 401/1000\n",
      "296/296 [==============================] - 0s 787us/step - loss: 1.2228 - accuracy: 0.5293 - val_loss: 1.3582 - val_accuracy: 0.4782\n",
      "Epoch 402/1000\n",
      "296/296 [==============================] - 0s 769us/step - loss: 1.1926 - accuracy: 0.5474 - val_loss: 1.3879 - val_accuracy: 0.4689\n",
      "Epoch 403/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.2018 - accuracy: 0.5408 - val_loss: 1.3544 - val_accuracy: 0.4774\n",
      "Epoch 404/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.2040 - accuracy: 0.5396 - val_loss: 1.3424 - val_accuracy: 0.4858\n",
      "Epoch 405/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2127 - accuracy: 0.5386 - val_loss: 1.3533 - val_accuracy: 0.4867\n",
      "Epoch 406/1000\n",
      "296/296 [==============================] - 0s 799us/step - loss: 1.1916 - accuracy: 0.5384 - val_loss: 1.3653 - val_accuracy: 0.4850\n",
      "Epoch 407/1000\n",
      "296/296 [==============================] - 0s 788us/step - loss: 1.2072 - accuracy: 0.5332 - val_loss: 1.3583 - val_accuracy: 0.4871\n",
      "Epoch 408/1000\n",
      "296/296 [==============================] - 0s 791us/step - loss: 1.2233 - accuracy: 0.5277 - val_loss: 1.3749 - val_accuracy: 0.4807\n",
      "Epoch 409/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.2036 - accuracy: 0.5394 - val_loss: 1.3727 - val_accuracy: 0.4824\n",
      "Epoch 410/1000\n",
      "296/296 [==============================] - 0s 754us/step - loss: 1.2027 - accuracy: 0.5392 - val_loss: 1.3633 - val_accuracy: 0.4846\n",
      "Epoch 411/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.2054 - accuracy: 0.5396 - val_loss: 1.3772 - val_accuracy: 0.4858\n",
      "Epoch 412/1000\n",
      "296/296 [==============================] - 0s 775us/step - loss: 1.1977 - accuracy: 0.5404 - val_loss: 1.3569 - val_accuracy: 0.4833\n",
      "Epoch 413/1000\n",
      "296/296 [==============================] - 0s 809us/step - loss: 1.2004 - accuracy: 0.5423 - val_loss: 1.3497 - val_accuracy: 0.4892\n",
      "Epoch 414/1000\n",
      "296/296 [==============================] - 0s 800us/step - loss: 1.2056 - accuracy: 0.5406 - val_loss: 1.3650 - val_accuracy: 0.4795\n",
      "Epoch 415/1000\n",
      "296/296 [==============================] - 0s 821us/step - loss: 1.2096 - accuracy: 0.5419 - val_loss: 1.3637 - val_accuracy: 0.4799\n",
      "Epoch 416/1000\n",
      "296/296 [==============================] - 0s 824us/step - loss: 1.1987 - accuracy: 0.5441 - val_loss: 1.3544 - val_accuracy: 0.4875\n",
      "Epoch 417/1000\n",
      "296/296 [==============================] - 0s 793us/step - loss: 1.1983 - accuracy: 0.5375 - val_loss: 1.3918 - val_accuracy: 0.4723\n",
      "Epoch 418/1000\n",
      "296/296 [==============================] - 0s 792us/step - loss: 1.1847 - accuracy: 0.5441 - val_loss: 1.3709 - val_accuracy: 0.4858\n",
      "Epoch 419/1000\n",
      "296/296 [==============================] - 0s 828us/step - loss: 1.2041 - accuracy: 0.5405 - val_loss: 1.3629 - val_accuracy: 0.4816\n",
      "Epoch 420/1000\n",
      "296/296 [==============================] - 0s 783us/step - loss: 1.2048 - accuracy: 0.5428 - val_loss: 1.3727 - val_accuracy: 0.4846\n",
      "Epoch 421/1000\n",
      "296/296 [==============================] - 0s 763us/step - loss: 1.2015 - accuracy: 0.5430 - val_loss: 1.3653 - val_accuracy: 0.4917\n",
      "Epoch 422/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.2050 - accuracy: 0.5404 - val_loss: 1.3772 - val_accuracy: 0.4824\n",
      "Epoch 423/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.2080 - accuracy: 0.5375 - val_loss: 1.3563 - val_accuracy: 0.4901\n",
      "Epoch 424/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.2050 - accuracy: 0.5331 - val_loss: 1.3660 - val_accuracy: 0.4820\n",
      "Epoch 425/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.1997 - accuracy: 0.5418 - val_loss: 1.3628 - val_accuracy: 0.4752\n",
      "Epoch 426/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.2000 - accuracy: 0.5390 - val_loss: 1.3681 - val_accuracy: 0.4748\n",
      "Epoch 427/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1979 - accuracy: 0.5353 - val_loss: 1.3436 - val_accuracy: 0.4867\n",
      "Epoch 428/1000\n",
      "296/296 [==============================] - 0s 777us/step - loss: 1.1885 - accuracy: 0.5432 - val_loss: 1.3682 - val_accuracy: 0.4820\n",
      "Epoch 429/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.2011 - accuracy: 0.5432 - val_loss: 1.3835 - val_accuracy: 0.4833\n",
      "Epoch 430/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2034 - accuracy: 0.5436 - val_loss: 1.3637 - val_accuracy: 0.4786\n",
      "Epoch 431/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1918 - accuracy: 0.5403 - val_loss: 1.3579 - val_accuracy: 0.4807\n",
      "Epoch 432/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1870 - accuracy: 0.5450 - val_loss: 1.3574 - val_accuracy: 0.4837\n",
      "Epoch 433/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.2031 - accuracy: 0.5392 - val_loss: 1.3622 - val_accuracy: 0.4812\n",
      "Epoch 434/1000\n",
      "296/296 [==============================] - 0s 797us/step - loss: 1.1990 - accuracy: 0.5462 - val_loss: 1.3553 - val_accuracy: 0.4829\n",
      "Epoch 435/1000\n",
      "296/296 [==============================] - 0s 764us/step - loss: 1.1936 - accuracy: 0.5440 - val_loss: 1.3707 - val_accuracy: 0.4884\n",
      "Epoch 436/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1906 - accuracy: 0.5483 - val_loss: 1.3676 - val_accuracy: 0.4752\n",
      "Epoch 437/1000\n",
      "296/296 [==============================] - 0s 775us/step - loss: 1.2136 - accuracy: 0.5353 - val_loss: 1.3820 - val_accuracy: 0.4761\n",
      "Epoch 438/1000\n",
      "296/296 [==============================] - 0s 769us/step - loss: 1.1914 - accuracy: 0.5395 - val_loss: 1.3630 - val_accuracy: 0.4807\n",
      "Epoch 439/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.1964 - accuracy: 0.5415 - val_loss: 1.3666 - val_accuracy: 0.4913\n",
      "Epoch 440/1000\n",
      "296/296 [==============================] - 0s 794us/step - loss: 1.2033 - accuracy: 0.5395 - val_loss: 1.3681 - val_accuracy: 0.4934\n",
      "Epoch 441/1000\n",
      "296/296 [==============================] - 0s 805us/step - loss: 1.1972 - accuracy: 0.5401 - val_loss: 1.3607 - val_accuracy: 0.4816\n",
      "Epoch 442/1000\n",
      "296/296 [==============================] - 0s 806us/step - loss: 1.1942 - accuracy: 0.5369 - val_loss: 1.3440 - val_accuracy: 0.4909\n",
      "Epoch 443/1000\n",
      "296/296 [==============================] - 0s 791us/step - loss: 1.1982 - accuracy: 0.5462 - val_loss: 1.3591 - val_accuracy: 0.4905\n",
      "Epoch 444/1000\n",
      "296/296 [==============================] - 0s 781us/step - loss: 1.1989 - accuracy: 0.5411 - val_loss: 1.3540 - val_accuracy: 0.4892\n",
      "Epoch 445/1000\n",
      "296/296 [==============================] - 0s 764us/step - loss: 1.1982 - accuracy: 0.5414 - val_loss: 1.3518 - val_accuracy: 0.4833\n",
      "Epoch 446/1000\n",
      "296/296 [==============================] - 0s 805us/step - loss: 1.1849 - accuracy: 0.5473 - val_loss: 1.3590 - val_accuracy: 0.4879\n",
      "Epoch 447/1000\n",
      "296/296 [==============================] - 0s 781us/step - loss: 1.1917 - accuracy: 0.5430 - val_loss: 1.3437 - val_accuracy: 0.4875\n",
      "Epoch 448/1000\n",
      "296/296 [==============================] - 0s 783us/step - loss: 1.1893 - accuracy: 0.5412 - val_loss: 1.3595 - val_accuracy: 0.4846\n",
      "Epoch 449/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1998 - accuracy: 0.5436 - val_loss: 1.3612 - val_accuracy: 0.4901\n",
      "Epoch 450/1000\n",
      "296/296 [==============================] - 0s 782us/step - loss: 1.1978 - accuracy: 0.5399 - val_loss: 1.3779 - val_accuracy: 0.4829\n",
      "Epoch 451/1000\n",
      "296/296 [==============================] - 0s 788us/step - loss: 1.1961 - accuracy: 0.5415 - val_loss: 1.3760 - val_accuracy: 0.4752\n",
      "Epoch 452/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.2195 - accuracy: 0.5340 - val_loss: 1.3677 - val_accuracy: 0.4761\n",
      "Epoch 453/1000\n",
      "296/296 [==============================] - 0s 781us/step - loss: 1.1919 - accuracy: 0.5471 - val_loss: 1.3773 - val_accuracy: 0.4846\n",
      "Epoch 454/1000\n",
      "296/296 [==============================] - 0s 765us/step - loss: 1.1908 - accuracy: 0.5420 - val_loss: 1.3685 - val_accuracy: 0.4820\n",
      "Epoch 455/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.2031 - accuracy: 0.5439 - val_loss: 1.3709 - val_accuracy: 0.4820\n",
      "Epoch 456/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1919 - accuracy: 0.5408 - val_loss: 1.3534 - val_accuracy: 0.4820\n",
      "Epoch 457/1000\n",
      "296/296 [==============================] - 0s 783us/step - loss: 1.1865 - accuracy: 0.5447 - val_loss: 1.4037 - val_accuracy: 0.4676\n",
      "Epoch 458/1000\n",
      "296/296 [==============================] - 0s 792us/step - loss: 1.1877 - accuracy: 0.5440 - val_loss: 1.3643 - val_accuracy: 0.4752\n",
      "Epoch 459/1000\n",
      "296/296 [==============================] - 0s 777us/step - loss: 1.1812 - accuracy: 0.5455 - val_loss: 1.3862 - val_accuracy: 0.4791\n",
      "Epoch 460/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.2018 - accuracy: 0.5396 - val_loss: 1.3709 - val_accuracy: 0.4689\n",
      "Epoch 461/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2019 - accuracy: 0.5424 - val_loss: 1.3786 - val_accuracy: 0.4761\n",
      "Epoch 462/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1968 - accuracy: 0.5408 - val_loss: 1.3930 - val_accuracy: 0.4757\n",
      "Epoch 463/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1938 - accuracy: 0.5516 - val_loss: 1.3790 - val_accuracy: 0.4697\n",
      "Epoch 464/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1964 - accuracy: 0.5441 - val_loss: 1.3818 - val_accuracy: 0.4786\n",
      "Epoch 465/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1916 - accuracy: 0.5462 - val_loss: 1.3892 - val_accuracy: 0.4761\n",
      "Epoch 466/1000\n",
      "296/296 [==============================] - 0s 770us/step - loss: 1.1716 - accuracy: 0.5508 - val_loss: 1.4238 - val_accuracy: 0.4727\n",
      "Epoch 467/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.1972 - accuracy: 0.5438 - val_loss: 1.3886 - val_accuracy: 0.4795\n",
      "Epoch 468/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1980 - accuracy: 0.5405 - val_loss: 1.3921 - val_accuracy: 0.4778\n",
      "Epoch 469/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.2003 - accuracy: 0.5440 - val_loss: 1.3930 - val_accuracy: 0.4736\n",
      "Epoch 470/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1782 - accuracy: 0.5440 - val_loss: 1.4049 - val_accuracy: 0.4846\n",
      "Epoch 471/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1909 - accuracy: 0.5491 - val_loss: 1.3765 - val_accuracy: 0.4879\n",
      "Epoch 472/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.2005 - accuracy: 0.5421 - val_loss: 1.3930 - val_accuracy: 0.4710\n",
      "Epoch 473/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1909 - accuracy: 0.5472 - val_loss: 1.4076 - val_accuracy: 0.4761\n",
      "Epoch 474/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.2034 - accuracy: 0.5474 - val_loss: 1.3971 - val_accuracy: 0.4731\n",
      "Epoch 475/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1972 - accuracy: 0.5428 - val_loss: 1.3906 - val_accuracy: 0.4799\n",
      "Epoch 476/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1949 - accuracy: 0.5415 - val_loss: 1.3734 - val_accuracy: 0.4752\n",
      "Epoch 477/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1923 - accuracy: 0.5444 - val_loss: 1.3713 - val_accuracy: 0.4795\n",
      "Epoch 478/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1911 - accuracy: 0.5421 - val_loss: 1.3702 - val_accuracy: 0.4837\n",
      "Epoch 479/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1872 - accuracy: 0.5446 - val_loss: 1.3822 - val_accuracy: 0.4812\n",
      "Epoch 480/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1963 - accuracy: 0.5451 - val_loss: 1.3756 - val_accuracy: 0.4752\n",
      "Epoch 481/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1897 - accuracy: 0.5459 - val_loss: 1.3655 - val_accuracy: 0.4799\n",
      "Epoch 482/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1878 - accuracy: 0.5434 - val_loss: 1.4182 - val_accuracy: 0.4668\n",
      "Epoch 483/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1918 - accuracy: 0.5488 - val_loss: 1.4007 - val_accuracy: 0.4744\n",
      "Epoch 484/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.1986 - accuracy: 0.5444 - val_loss: 1.3923 - val_accuracy: 0.4782\n",
      "Epoch 485/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1987 - accuracy: 0.5425 - val_loss: 1.3841 - val_accuracy: 0.4816\n",
      "Epoch 486/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1793 - accuracy: 0.5531 - val_loss: 1.3804 - val_accuracy: 0.4736\n",
      "Epoch 487/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1919 - accuracy: 0.5442 - val_loss: 1.3794 - val_accuracy: 0.4867\n",
      "Epoch 488/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1786 - accuracy: 0.5477 - val_loss: 1.3896 - val_accuracy: 0.4774\n",
      "Epoch 489/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.2005 - accuracy: 0.5523 - val_loss: 1.3914 - val_accuracy: 0.4812\n",
      "Epoch 490/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.1979 - accuracy: 0.5411 - val_loss: 1.3774 - val_accuracy: 0.4837\n",
      "Epoch 491/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1888 - accuracy: 0.5467 - val_loss: 1.3581 - val_accuracy: 0.4846\n",
      "Epoch 492/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1966 - accuracy: 0.5407 - val_loss: 1.3584 - val_accuracy: 0.4816\n",
      "Epoch 493/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1831 - accuracy: 0.5466 - val_loss: 1.3622 - val_accuracy: 0.4736\n",
      "Epoch 494/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1930 - accuracy: 0.5530 - val_loss: 1.3529 - val_accuracy: 0.4824\n",
      "Epoch 495/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1893 - accuracy: 0.5456 - val_loss: 1.3708 - val_accuracy: 0.4862\n",
      "Epoch 496/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1861 - accuracy: 0.5460 - val_loss: 1.3879 - val_accuracy: 0.4706\n",
      "Epoch 497/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1696 - accuracy: 0.5506 - val_loss: 1.3878 - val_accuracy: 0.4824\n",
      "Epoch 498/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1946 - accuracy: 0.5418 - val_loss: 1.3698 - val_accuracy: 0.4765\n",
      "Epoch 499/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1711 - accuracy: 0.5472 - val_loss: 1.4060 - val_accuracy: 0.4791\n",
      "Epoch 500/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1813 - accuracy: 0.5388 - val_loss: 1.3984 - val_accuracy: 0.4824\n",
      "Epoch 501/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.2010 - accuracy: 0.5405 - val_loss: 1.3732 - val_accuracy: 0.4769\n",
      "Epoch 502/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.1843 - accuracy: 0.5489 - val_loss: 1.3791 - val_accuracy: 0.4714\n",
      "Epoch 503/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1968 - accuracy: 0.5457 - val_loss: 1.3814 - val_accuracy: 0.4702\n",
      "Epoch 504/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.2047 - accuracy: 0.5360 - val_loss: 1.3751 - val_accuracy: 0.4829\n",
      "Epoch 505/1000\n",
      "296/296 [==============================] - 0s 803us/step - loss: 1.1823 - accuracy: 0.5503 - val_loss: 1.4032 - val_accuracy: 0.4769\n",
      "Epoch 506/1000\n",
      "296/296 [==============================] - 0s 765us/step - loss: 1.1933 - accuracy: 0.5436 - val_loss: 1.3640 - val_accuracy: 0.4795\n",
      "Epoch 507/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.1826 - accuracy: 0.5499 - val_loss: 1.3717 - val_accuracy: 0.4816\n",
      "Epoch 508/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1970 - accuracy: 0.5491 - val_loss: 1.4083 - val_accuracy: 0.4752\n",
      "Epoch 509/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1889 - accuracy: 0.5470 - val_loss: 1.3791 - val_accuracy: 0.4846\n",
      "Epoch 510/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1902 - accuracy: 0.5438 - val_loss: 1.3840 - val_accuracy: 0.4871\n",
      "Epoch 511/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.1848 - accuracy: 0.5450 - val_loss: 1.4120 - val_accuracy: 0.4807\n",
      "Epoch 512/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1928 - accuracy: 0.5484 - val_loss: 1.3860 - val_accuracy: 0.4841\n",
      "Epoch 513/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1856 - accuracy: 0.5475 - val_loss: 1.3885 - val_accuracy: 0.4731\n",
      "Epoch 514/1000\n",
      "296/296 [==============================] - 0s 789us/step - loss: 1.1956 - accuracy: 0.5431 - val_loss: 1.3822 - val_accuracy: 0.4693\n",
      "Epoch 515/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1904 - accuracy: 0.5439 - val_loss: 1.3699 - val_accuracy: 0.4812\n",
      "Epoch 516/1000\n",
      "296/296 [==============================] - 0s 759us/step - loss: 1.1887 - accuracy: 0.5459 - val_loss: 1.4047 - val_accuracy: 0.4795\n",
      "Epoch 517/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1949 - accuracy: 0.5451 - val_loss: 1.3825 - val_accuracy: 0.4740\n",
      "Epoch 518/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1853 - accuracy: 0.5493 - val_loss: 1.3987 - val_accuracy: 0.4761\n",
      "Epoch 519/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1977 - accuracy: 0.5480 - val_loss: 1.3962 - val_accuracy: 0.4846\n",
      "Epoch 520/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1823 - accuracy: 0.5556 - val_loss: 1.3954 - val_accuracy: 0.4757\n",
      "Epoch 521/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1944 - accuracy: 0.5480 - val_loss: 1.3988 - val_accuracy: 0.4765\n",
      "Epoch 522/1000\n",
      "296/296 [==============================] - 0s 761us/step - loss: 1.1824 - accuracy: 0.5525 - val_loss: 1.3762 - val_accuracy: 0.4816\n",
      "Epoch 523/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1951 - accuracy: 0.5433 - val_loss: 1.3918 - val_accuracy: 0.4799\n",
      "Epoch 524/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.2059 - accuracy: 0.5418 - val_loss: 1.4029 - val_accuracy: 0.4685\n",
      "Epoch 525/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1941 - accuracy: 0.5469 - val_loss: 1.4017 - val_accuracy: 0.4693\n",
      "Epoch 526/1000\n",
      "296/296 [==============================] - 0s 819us/step - loss: 1.2057 - accuracy: 0.5408 - val_loss: 1.3748 - val_accuracy: 0.4858\n",
      "Epoch 527/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.1978 - accuracy: 0.5389 - val_loss: 1.3948 - val_accuracy: 0.4791\n",
      "Epoch 528/1000\n",
      "296/296 [==============================] - 0s 763us/step - loss: 1.1898 - accuracy: 0.5467 - val_loss: 1.3690 - val_accuracy: 0.4736\n",
      "Epoch 529/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1860 - accuracy: 0.5483 - val_loss: 1.3703 - val_accuracy: 0.4786\n",
      "Epoch 530/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1898 - accuracy: 0.5519 - val_loss: 1.3823 - val_accuracy: 0.4702\n",
      "Epoch 531/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1971 - accuracy: 0.5465 - val_loss: 1.3814 - val_accuracy: 0.4824\n",
      "Epoch 532/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1846 - accuracy: 0.5400 - val_loss: 1.4010 - val_accuracy: 0.4757\n",
      "Epoch 533/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.1878 - accuracy: 0.5412 - val_loss: 1.3731 - val_accuracy: 0.4642\n",
      "Epoch 534/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.1872 - accuracy: 0.5467 - val_loss: 1.3869 - val_accuracy: 0.4786\n",
      "Epoch 535/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1839 - accuracy: 0.5463 - val_loss: 1.3918 - val_accuracy: 0.4782\n",
      "Epoch 536/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1794 - accuracy: 0.5453 - val_loss: 1.3720 - val_accuracy: 0.4668\n",
      "Epoch 537/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.2038 - accuracy: 0.5451 - val_loss: 1.3797 - val_accuracy: 0.4757\n",
      "Epoch 538/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.1950 - accuracy: 0.5490 - val_loss: 1.3697 - val_accuracy: 0.4812\n",
      "Epoch 539/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1804 - accuracy: 0.5526 - val_loss: 1.3825 - val_accuracy: 0.4727\n",
      "Epoch 540/1000\n",
      "296/296 [==============================] - 0s 758us/step - loss: 1.1871 - accuracy: 0.5389 - val_loss: 1.4043 - val_accuracy: 0.4740\n",
      "Epoch 541/1000\n",
      "296/296 [==============================] - 0s 758us/step - loss: 1.1921 - accuracy: 0.5498 - val_loss: 1.4073 - val_accuracy: 0.4689\n",
      "Epoch 542/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1968 - accuracy: 0.5494 - val_loss: 1.3925 - val_accuracy: 0.4604\n",
      "Epoch 543/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1893 - accuracy: 0.5472 - val_loss: 1.3686 - val_accuracy: 0.4879\n",
      "Epoch 544/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1808 - accuracy: 0.5454 - val_loss: 1.3924 - val_accuracy: 0.4731\n",
      "Epoch 545/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1881 - accuracy: 0.5513 - val_loss: 1.3669 - val_accuracy: 0.4850\n",
      "Epoch 546/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1901 - accuracy: 0.5550 - val_loss: 1.3754 - val_accuracy: 0.4854\n",
      "Epoch 547/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1949 - accuracy: 0.5441 - val_loss: 1.3875 - val_accuracy: 0.4833\n",
      "Epoch 548/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1967 - accuracy: 0.5539 - val_loss: 1.3731 - val_accuracy: 0.4816\n",
      "Epoch 549/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1927 - accuracy: 0.5487 - val_loss: 1.3655 - val_accuracy: 0.4761\n",
      "Epoch 550/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1799 - accuracy: 0.5465 - val_loss: 1.3697 - val_accuracy: 0.4824\n",
      "Epoch 551/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1944 - accuracy: 0.5466 - val_loss: 1.3933 - val_accuracy: 0.4761\n",
      "Epoch 552/1000\n",
      "296/296 [==============================] - 0s 789us/step - loss: 1.1956 - accuracy: 0.5405 - val_loss: 1.3750 - val_accuracy: 0.4744\n",
      "Epoch 553/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.1794 - accuracy: 0.5416 - val_loss: 1.3903 - val_accuracy: 0.4774\n",
      "Epoch 554/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1915 - accuracy: 0.5476 - val_loss: 1.3685 - val_accuracy: 0.4846\n",
      "Epoch 555/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1873 - accuracy: 0.5504 - val_loss: 1.3742 - val_accuracy: 0.4680\n",
      "Epoch 556/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.2058 - accuracy: 0.5453 - val_loss: 1.3731 - val_accuracy: 0.4757\n",
      "Epoch 557/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1987 - accuracy: 0.5393 - val_loss: 1.3676 - val_accuracy: 0.4752\n",
      "Epoch 558/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1878 - accuracy: 0.5426 - val_loss: 1.3836 - val_accuracy: 0.4727\n",
      "Epoch 559/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1842 - accuracy: 0.5531 - val_loss: 1.3922 - val_accuracy: 0.4824\n",
      "Epoch 560/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1804 - accuracy: 0.5492 - val_loss: 1.4210 - val_accuracy: 0.4769\n",
      "Epoch 561/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1949 - accuracy: 0.5426 - val_loss: 1.4079 - val_accuracy: 0.4736\n",
      "Epoch 562/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1898 - accuracy: 0.5516 - val_loss: 1.3792 - val_accuracy: 0.4837\n",
      "Epoch 563/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1861 - accuracy: 0.5477 - val_loss: 1.3938 - val_accuracy: 0.4824\n",
      "Epoch 564/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1889 - accuracy: 0.5453 - val_loss: 1.4101 - val_accuracy: 0.4655\n",
      "Epoch 565/1000\n",
      "296/296 [==============================] - 0s 759us/step - loss: 1.2004 - accuracy: 0.5394 - val_loss: 1.4096 - val_accuracy: 0.4752\n",
      "Epoch 566/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1943 - accuracy: 0.5512 - val_loss: 1.4108 - val_accuracy: 0.4774\n",
      "Epoch 567/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1906 - accuracy: 0.5437 - val_loss: 1.3969 - val_accuracy: 0.4778\n",
      "Epoch 568/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1819 - accuracy: 0.5491 - val_loss: 1.3907 - val_accuracy: 0.4816\n",
      "Epoch 569/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1875 - accuracy: 0.5454 - val_loss: 1.3979 - val_accuracy: 0.4778\n",
      "Epoch 570/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1853 - accuracy: 0.5446 - val_loss: 1.3833 - val_accuracy: 0.4803\n",
      "Epoch 571/1000\n",
      "296/296 [==============================] - 0s 758us/step - loss: 1.1795 - accuracy: 0.5440 - val_loss: 1.4118 - val_accuracy: 0.4710\n",
      "Epoch 572/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.1945 - accuracy: 0.5480 - val_loss: 1.3838 - val_accuracy: 0.4714\n",
      "Epoch 573/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1980 - accuracy: 0.5376 - val_loss: 1.4238 - val_accuracy: 0.4697\n",
      "Epoch 574/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1815 - accuracy: 0.5476 - val_loss: 1.4023 - val_accuracy: 0.4740\n",
      "Epoch 575/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1905 - accuracy: 0.5439 - val_loss: 1.3980 - val_accuracy: 0.4871\n",
      "Epoch 576/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1894 - accuracy: 0.5493 - val_loss: 1.3819 - val_accuracy: 0.4829\n",
      "Epoch 577/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1888 - accuracy: 0.5490 - val_loss: 1.3861 - val_accuracy: 0.4769\n",
      "Epoch 578/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1876 - accuracy: 0.5450 - val_loss: 1.3875 - val_accuracy: 0.4736\n",
      "Epoch 579/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.1907 - accuracy: 0.5467 - val_loss: 1.4258 - val_accuracy: 0.4871\n",
      "Epoch 580/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1800 - accuracy: 0.5493 - val_loss: 1.3834 - val_accuracy: 0.4824\n",
      "Epoch 581/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1912 - accuracy: 0.5480 - val_loss: 1.4098 - val_accuracy: 0.4642\n",
      "Epoch 582/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1729 - accuracy: 0.5550 - val_loss: 1.4030 - val_accuracy: 0.4765\n",
      "Epoch 583/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1808 - accuracy: 0.5535 - val_loss: 1.4030 - val_accuracy: 0.4706\n",
      "Epoch 584/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1865 - accuracy: 0.5496 - val_loss: 1.3862 - val_accuracy: 0.4748\n",
      "Epoch 585/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1774 - accuracy: 0.5548 - val_loss: 1.4133 - val_accuracy: 0.4702\n",
      "Epoch 586/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1907 - accuracy: 0.5424 - val_loss: 1.3893 - val_accuracy: 0.4791\n",
      "Epoch 587/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1840 - accuracy: 0.5503 - val_loss: 1.4059 - val_accuracy: 0.4731\n",
      "Epoch 588/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1937 - accuracy: 0.5454 - val_loss: 1.3770 - val_accuracy: 0.4782\n",
      "Epoch 589/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1958 - accuracy: 0.5508 - val_loss: 1.3935 - val_accuracy: 0.4693\n",
      "Epoch 590/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1848 - accuracy: 0.5491 - val_loss: 1.3858 - val_accuracy: 0.4748\n",
      "Epoch 591/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1863 - accuracy: 0.5425 - val_loss: 1.4107 - val_accuracy: 0.4744\n",
      "Epoch 592/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1844 - accuracy: 0.5468 - val_loss: 1.3983 - val_accuracy: 0.4685\n",
      "Epoch 593/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.1809 - accuracy: 0.5498 - val_loss: 1.3753 - val_accuracy: 0.4829\n",
      "Epoch 594/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1752 - accuracy: 0.5481 - val_loss: 1.4056 - val_accuracy: 0.4778\n",
      "Epoch 595/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1846 - accuracy: 0.5457 - val_loss: 1.3991 - val_accuracy: 0.4706\n",
      "Epoch 596/1000\n",
      "296/296 [==============================] - 0s 754us/step - loss: 1.1745 - accuracy: 0.5538 - val_loss: 1.4036 - val_accuracy: 0.4778\n",
      "Epoch 597/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1695 - accuracy: 0.5498 - val_loss: 1.4036 - val_accuracy: 0.4689\n",
      "Epoch 598/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1965 - accuracy: 0.5523 - val_loss: 1.3923 - val_accuracy: 0.4714\n",
      "Epoch 599/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1907 - accuracy: 0.5510 - val_loss: 1.4014 - val_accuracy: 0.4680\n",
      "Epoch 600/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1893 - accuracy: 0.5428 - val_loss: 1.4052 - val_accuracy: 0.4647\n",
      "Epoch 601/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1771 - accuracy: 0.5502 - val_loss: 1.3708 - val_accuracy: 0.4740\n",
      "Epoch 602/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1890 - accuracy: 0.5490 - val_loss: 1.3852 - val_accuracy: 0.4757\n",
      "Epoch 603/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.2021 - accuracy: 0.5455 - val_loss: 1.3979 - val_accuracy: 0.4680\n",
      "Epoch 604/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1773 - accuracy: 0.5484 - val_loss: 1.4083 - val_accuracy: 0.4693\n",
      "Epoch 605/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1943 - accuracy: 0.5465 - val_loss: 1.3999 - val_accuracy: 0.4744\n",
      "Epoch 606/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1844 - accuracy: 0.5543 - val_loss: 1.4215 - val_accuracy: 0.4727\n",
      "Epoch 607/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.1707 - accuracy: 0.5590 - val_loss: 1.4176 - val_accuracy: 0.4714\n",
      "Epoch 608/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1840 - accuracy: 0.5535 - val_loss: 1.4163 - val_accuracy: 0.4714\n",
      "Epoch 609/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.1791 - accuracy: 0.5546 - val_loss: 1.3977 - val_accuracy: 0.4752\n",
      "Epoch 610/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1887 - accuracy: 0.5466 - val_loss: 1.3912 - val_accuracy: 0.4820\n",
      "Epoch 611/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1712 - accuracy: 0.5493 - val_loss: 1.4233 - val_accuracy: 0.4693\n",
      "Epoch 612/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.1901 - accuracy: 0.5525 - val_loss: 1.4003 - val_accuracy: 0.4757\n",
      "Epoch 613/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1693 - accuracy: 0.5538 - val_loss: 1.3978 - val_accuracy: 0.4905\n",
      "Epoch 614/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1858 - accuracy: 0.5480 - val_loss: 1.4165 - val_accuracy: 0.4791\n",
      "Epoch 615/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1892 - accuracy: 0.5470 - val_loss: 1.4035 - val_accuracy: 0.4659\n",
      "Epoch 616/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.1860 - accuracy: 0.5499 - val_loss: 1.4144 - val_accuracy: 0.4761\n",
      "Epoch 617/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1808 - accuracy: 0.5539 - val_loss: 1.4153 - val_accuracy: 0.4782\n",
      "Epoch 618/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1814 - accuracy: 0.5517 - val_loss: 1.4127 - val_accuracy: 0.4757\n",
      "Epoch 619/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1881 - accuracy: 0.5463 - val_loss: 1.3931 - val_accuracy: 0.4799\n",
      "Epoch 620/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.1716 - accuracy: 0.5525 - val_loss: 1.3945 - val_accuracy: 0.4807\n",
      "Epoch 621/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1851 - accuracy: 0.5488 - val_loss: 1.4320 - val_accuracy: 0.4549\n",
      "Epoch 622/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1775 - accuracy: 0.5532 - val_loss: 1.4085 - val_accuracy: 0.4765\n",
      "Epoch 623/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1871 - accuracy: 0.5508 - val_loss: 1.4038 - val_accuracy: 0.4850\n",
      "Epoch 624/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1888 - accuracy: 0.5480 - val_loss: 1.4085 - val_accuracy: 0.4638\n",
      "Epoch 625/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1855 - accuracy: 0.5498 - val_loss: 1.3954 - val_accuracy: 0.4812\n",
      "Epoch 626/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.1746 - accuracy: 0.5485 - val_loss: 1.4097 - val_accuracy: 0.4685\n",
      "Epoch 627/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1797 - accuracy: 0.5527 - val_loss: 1.4028 - val_accuracy: 0.4689\n",
      "Epoch 628/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.1832 - accuracy: 0.5499 - val_loss: 1.4079 - val_accuracy: 0.4664\n",
      "Epoch 629/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1825 - accuracy: 0.5483 - val_loss: 1.4274 - val_accuracy: 0.4765\n",
      "Epoch 630/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1779 - accuracy: 0.5496 - val_loss: 1.4010 - val_accuracy: 0.4757\n",
      "Epoch 631/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1823 - accuracy: 0.5537 - val_loss: 1.3997 - val_accuracy: 0.4706\n",
      "Epoch 632/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1751 - accuracy: 0.5514 - val_loss: 1.4089 - val_accuracy: 0.4723\n",
      "Epoch 633/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1835 - accuracy: 0.5495 - val_loss: 1.3984 - val_accuracy: 0.4719\n",
      "Epoch 634/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.1791 - accuracy: 0.5507 - val_loss: 1.3842 - val_accuracy: 0.4757\n",
      "Epoch 635/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.1857 - accuracy: 0.5546 - val_loss: 1.3886 - val_accuracy: 0.4778\n",
      "Epoch 636/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1826 - accuracy: 0.5520 - val_loss: 1.3980 - val_accuracy: 0.4744\n",
      "Epoch 637/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1825 - accuracy: 0.5521 - val_loss: 1.3860 - val_accuracy: 0.4812\n",
      "Epoch 638/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.1870 - accuracy: 0.5540 - val_loss: 1.4033 - val_accuracy: 0.4731\n",
      "Epoch 639/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1689 - accuracy: 0.5527 - val_loss: 1.3770 - val_accuracy: 0.4803\n",
      "Epoch 640/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1665 - accuracy: 0.5494 - val_loss: 1.4028 - val_accuracy: 0.4833\n",
      "Epoch 641/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1778 - accuracy: 0.5568 - val_loss: 1.3912 - val_accuracy: 0.4719\n",
      "Epoch 642/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1700 - accuracy: 0.5561 - val_loss: 1.3834 - val_accuracy: 0.4858\n",
      "Epoch 643/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1768 - accuracy: 0.5521 - val_loss: 1.4047 - val_accuracy: 0.4655\n",
      "Epoch 644/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1760 - accuracy: 0.5568 - val_loss: 1.4105 - val_accuracy: 0.4791\n",
      "Epoch 645/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.2020 - accuracy: 0.5512 - val_loss: 1.3838 - val_accuracy: 0.4812\n",
      "Epoch 646/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1878 - accuracy: 0.5486 - val_loss: 1.3877 - val_accuracy: 0.4820\n",
      "Epoch 647/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1804 - accuracy: 0.5511 - val_loss: 1.3890 - val_accuracy: 0.4769\n",
      "Epoch 648/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1776 - accuracy: 0.5515 - val_loss: 1.4054 - val_accuracy: 0.4702\n",
      "Epoch 649/1000\n",
      "296/296 [==============================] - 0s 717us/step - loss: 1.1776 - accuracy: 0.5566 - val_loss: 1.3925 - val_accuracy: 0.4757\n",
      "Epoch 650/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1880 - accuracy: 0.5510 - val_loss: 1.3908 - val_accuracy: 0.4812\n",
      "Epoch 651/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1865 - accuracy: 0.5491 - val_loss: 1.3905 - val_accuracy: 0.4778\n",
      "Epoch 652/1000\n",
      "296/296 [==============================] - 0s 772us/step - loss: 1.1799 - accuracy: 0.5476 - val_loss: 1.3939 - val_accuracy: 0.4879\n",
      "Epoch 653/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1794 - accuracy: 0.5431 - val_loss: 1.4130 - val_accuracy: 0.4807\n",
      "Epoch 654/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1895 - accuracy: 0.5456 - val_loss: 1.4084 - val_accuracy: 0.4791\n",
      "Epoch 655/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.1807 - accuracy: 0.5559 - val_loss: 1.4026 - val_accuracy: 0.4833\n",
      "Epoch 656/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.1755 - accuracy: 0.5585 - val_loss: 1.4328 - val_accuracy: 0.4702\n",
      "Epoch 657/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1931 - accuracy: 0.5456 - val_loss: 1.4172 - val_accuracy: 0.4651\n",
      "Epoch 658/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1887 - accuracy: 0.5471 - val_loss: 1.3968 - val_accuracy: 0.4799\n",
      "Epoch 659/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1788 - accuracy: 0.5598 - val_loss: 1.4053 - val_accuracy: 0.4778\n",
      "Epoch 660/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1858 - accuracy: 0.5473 - val_loss: 1.4040 - val_accuracy: 0.4757\n",
      "Epoch 661/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1715 - accuracy: 0.5564 - val_loss: 1.4078 - val_accuracy: 0.4824\n",
      "Epoch 662/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1924 - accuracy: 0.5486 - val_loss: 1.3858 - val_accuracy: 0.4803\n",
      "Epoch 663/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.1913 - accuracy: 0.5530 - val_loss: 1.3934 - val_accuracy: 0.4676\n",
      "Epoch 664/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1793 - accuracy: 0.5506 - val_loss: 1.4044 - val_accuracy: 0.4752\n",
      "Epoch 665/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1705 - accuracy: 0.5538 - val_loss: 1.4116 - val_accuracy: 0.4791\n",
      "Epoch 666/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1640 - accuracy: 0.5564 - val_loss: 1.4141 - val_accuracy: 0.4778\n",
      "Epoch 667/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1906 - accuracy: 0.5563 - val_loss: 1.4090 - val_accuracy: 0.4795\n",
      "Epoch 668/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1723 - accuracy: 0.5530 - val_loss: 1.4212 - val_accuracy: 0.4812\n",
      "Epoch 669/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1876 - accuracy: 0.5453 - val_loss: 1.4278 - val_accuracy: 0.4744\n",
      "Epoch 670/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1802 - accuracy: 0.5534 - val_loss: 1.4182 - val_accuracy: 0.4837\n",
      "Epoch 671/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1818 - accuracy: 0.5457 - val_loss: 1.4184 - val_accuracy: 0.4731\n",
      "Epoch 672/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1644 - accuracy: 0.5570 - val_loss: 1.4136 - val_accuracy: 0.4829\n",
      "Epoch 673/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1801 - accuracy: 0.5539 - val_loss: 1.4054 - val_accuracy: 0.4867\n",
      "Epoch 674/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1838 - accuracy: 0.5483 - val_loss: 1.4065 - val_accuracy: 0.4820\n",
      "Epoch 675/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1686 - accuracy: 0.5571 - val_loss: 1.4106 - val_accuracy: 0.4812\n",
      "Epoch 676/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1907 - accuracy: 0.5477 - val_loss: 1.4053 - val_accuracy: 0.4841\n",
      "Epoch 677/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1762 - accuracy: 0.5514 - val_loss: 1.4003 - val_accuracy: 0.4816\n",
      "Epoch 678/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1803 - accuracy: 0.5570 - val_loss: 1.3733 - val_accuracy: 0.4884\n",
      "Epoch 679/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1771 - accuracy: 0.5523 - val_loss: 1.3928 - val_accuracy: 0.4824\n",
      "Epoch 680/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1780 - accuracy: 0.5625 - val_loss: 1.3845 - val_accuracy: 0.4820\n",
      "Epoch 681/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1691 - accuracy: 0.5557 - val_loss: 1.4182 - val_accuracy: 0.4795\n",
      "Epoch 682/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1788 - accuracy: 0.5511 - val_loss: 1.4239 - val_accuracy: 0.4710\n",
      "Epoch 683/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1740 - accuracy: 0.5550 - val_loss: 1.4273 - val_accuracy: 0.4829\n",
      "Epoch 684/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1843 - accuracy: 0.5486 - val_loss: 1.4411 - val_accuracy: 0.4693\n",
      "Epoch 685/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1836 - accuracy: 0.5471 - val_loss: 1.4201 - val_accuracy: 0.4850\n",
      "Epoch 686/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1892 - accuracy: 0.5492 - val_loss: 1.4470 - val_accuracy: 0.4807\n",
      "Epoch 687/1000\n",
      "296/296 [==============================] - 0s 767us/step - loss: 1.1821 - accuracy: 0.5510 - val_loss: 1.4135 - val_accuracy: 0.4812\n",
      "Epoch 688/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1890 - accuracy: 0.5477 - val_loss: 1.4368 - val_accuracy: 0.4816\n",
      "Epoch 689/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1770 - accuracy: 0.5535 - val_loss: 1.4232 - val_accuracy: 0.4689\n",
      "Epoch 690/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1784 - accuracy: 0.5548 - val_loss: 1.4182 - val_accuracy: 0.4748\n",
      "Epoch 691/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1918 - accuracy: 0.5497 - val_loss: 1.4044 - val_accuracy: 0.4799\n",
      "Epoch 692/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1871 - accuracy: 0.5521 - val_loss: 1.3961 - val_accuracy: 0.4816\n",
      "Epoch 693/1000\n",
      "296/296 [==============================] - 0s 722us/step - loss: 1.1816 - accuracy: 0.5493 - val_loss: 1.4034 - val_accuracy: 0.4786\n",
      "Epoch 694/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1751 - accuracy: 0.5602 - val_loss: 1.4017 - val_accuracy: 0.4791\n",
      "Epoch 695/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1827 - accuracy: 0.5515 - val_loss: 1.3899 - val_accuracy: 0.4778\n",
      "Epoch 696/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1729 - accuracy: 0.5529 - val_loss: 1.3842 - val_accuracy: 0.4841\n",
      "Epoch 697/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1788 - accuracy: 0.5521 - val_loss: 1.4385 - val_accuracy: 0.4765\n",
      "Epoch 698/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1797 - accuracy: 0.5544 - val_loss: 1.4244 - val_accuracy: 0.4744\n",
      "Epoch 699/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1629 - accuracy: 0.5590 - val_loss: 1.4108 - val_accuracy: 0.4714\n",
      "Epoch 700/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1792 - accuracy: 0.5572 - val_loss: 1.4008 - val_accuracy: 0.4680\n",
      "Epoch 701/1000\n",
      "296/296 [==============================] - 0s 786us/step - loss: 1.1842 - accuracy: 0.5506 - val_loss: 1.4000 - val_accuracy: 0.4795\n",
      "Epoch 702/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.1958 - accuracy: 0.5468 - val_loss: 1.4054 - val_accuracy: 0.4769\n",
      "Epoch 703/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.1797 - accuracy: 0.5572 - val_loss: 1.3972 - val_accuracy: 0.4799\n",
      "Epoch 704/1000\n",
      "296/296 [==============================] - 0s 774us/step - loss: 1.1871 - accuracy: 0.5549 - val_loss: 1.4091 - val_accuracy: 0.4829\n",
      "Epoch 705/1000\n",
      "296/296 [==============================] - 0s 760us/step - loss: 1.1820 - accuracy: 0.5558 - val_loss: 1.4107 - val_accuracy: 0.4812\n",
      "Epoch 706/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1923 - accuracy: 0.5595 - val_loss: 1.3990 - val_accuracy: 0.4820\n",
      "Epoch 707/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1946 - accuracy: 0.5548 - val_loss: 1.4131 - val_accuracy: 0.4791\n",
      "Epoch 708/1000\n",
      "296/296 [==============================] - 0s 764us/step - loss: 1.1830 - accuracy: 0.5506 - val_loss: 1.4238 - val_accuracy: 0.4710\n",
      "Epoch 709/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1871 - accuracy: 0.5481 - val_loss: 1.4223 - val_accuracy: 0.4740\n",
      "Epoch 710/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1983 - accuracy: 0.5534 - val_loss: 1.4192 - val_accuracy: 0.4829\n",
      "Epoch 711/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1764 - accuracy: 0.5504 - val_loss: 1.3929 - val_accuracy: 0.4731\n",
      "Epoch 712/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1766 - accuracy: 0.5509 - val_loss: 1.3989 - val_accuracy: 0.4879\n",
      "Epoch 713/1000\n",
      "296/296 [==============================] - 0s 759us/step - loss: 1.1744 - accuracy: 0.5565 - val_loss: 1.3934 - val_accuracy: 0.4871\n",
      "Epoch 714/1000\n",
      "296/296 [==============================] - 0s 778us/step - loss: 1.1747 - accuracy: 0.5579 - val_loss: 1.4120 - val_accuracy: 0.4879\n",
      "Epoch 715/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1795 - accuracy: 0.5557 - val_loss: 1.4184 - val_accuracy: 0.4740\n",
      "Epoch 716/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.1841 - accuracy: 0.5471 - val_loss: 1.3888 - val_accuracy: 0.4769\n",
      "Epoch 717/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1789 - accuracy: 0.5520 - val_loss: 1.3823 - val_accuracy: 0.4837\n",
      "Epoch 718/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1745 - accuracy: 0.5528 - val_loss: 1.4133 - val_accuracy: 0.4761\n",
      "Epoch 719/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1797 - accuracy: 0.5578 - val_loss: 1.3963 - val_accuracy: 0.4833\n",
      "Epoch 720/1000\n",
      "296/296 [==============================] - 0s 723us/step - loss: 1.1842 - accuracy: 0.5481 - val_loss: 1.4211 - val_accuracy: 0.4833\n",
      "Epoch 721/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1725 - accuracy: 0.5552 - val_loss: 1.3958 - val_accuracy: 0.4740\n",
      "Epoch 722/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1780 - accuracy: 0.5535 - val_loss: 1.3987 - val_accuracy: 0.4702\n",
      "Epoch 723/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1724 - accuracy: 0.5528 - val_loss: 1.3916 - val_accuracy: 0.4854\n",
      "Epoch 724/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1659 - accuracy: 0.5569 - val_loss: 1.4014 - val_accuracy: 0.4879\n",
      "Epoch 725/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1814 - accuracy: 0.5532 - val_loss: 1.3829 - val_accuracy: 0.4833\n",
      "Epoch 726/1000\n",
      "296/296 [==============================] - 0s 778us/step - loss: 1.1800 - accuracy: 0.5517 - val_loss: 1.3994 - val_accuracy: 0.4846\n",
      "Epoch 727/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1665 - accuracy: 0.5520 - val_loss: 1.4133 - val_accuracy: 0.4736\n",
      "Epoch 728/1000\n",
      "296/296 [==============================] - 0s 789us/step - loss: 1.1859 - accuracy: 0.5491 - val_loss: 1.4050 - val_accuracy: 0.4862\n",
      "Epoch 729/1000\n",
      "296/296 [==============================] - 0s 779us/step - loss: 1.1731 - accuracy: 0.5571 - val_loss: 1.3900 - val_accuracy: 0.4820\n",
      "Epoch 730/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1807 - accuracy: 0.5522 - val_loss: 1.3978 - val_accuracy: 0.4807\n",
      "Epoch 731/1000\n",
      "296/296 [==============================] - 0s 778us/step - loss: 1.1749 - accuracy: 0.5565 - val_loss: 1.4017 - val_accuracy: 0.4867\n",
      "Epoch 732/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1900 - accuracy: 0.5511 - val_loss: 1.4277 - val_accuracy: 0.4706\n",
      "Epoch 733/1000\n",
      "296/296 [==============================] - 0s 746us/step - loss: 1.1768 - accuracy: 0.5541 - val_loss: 1.4055 - val_accuracy: 0.4774\n",
      "Epoch 734/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1753 - accuracy: 0.5586 - val_loss: 1.4041 - val_accuracy: 0.4799\n",
      "Epoch 735/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1676 - accuracy: 0.5570 - val_loss: 1.4161 - val_accuracy: 0.4791\n",
      "Epoch 736/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1649 - accuracy: 0.5574 - val_loss: 1.4175 - val_accuracy: 0.4672\n",
      "Epoch 737/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.1770 - accuracy: 0.5541 - val_loss: 1.4010 - val_accuracy: 0.4744\n",
      "Epoch 738/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1980 - accuracy: 0.5510 - val_loss: 1.4308 - val_accuracy: 0.4647\n",
      "Epoch 739/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1660 - accuracy: 0.5531 - val_loss: 1.3891 - val_accuracy: 0.4901\n",
      "Epoch 740/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1669 - accuracy: 0.5531 - val_loss: 1.4008 - val_accuracy: 0.4748\n",
      "Epoch 741/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1630 - accuracy: 0.5597 - val_loss: 1.3947 - val_accuracy: 0.4807\n",
      "Epoch 742/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1739 - accuracy: 0.5535 - val_loss: 1.4073 - val_accuracy: 0.4752\n",
      "Epoch 743/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1664 - accuracy: 0.5546 - val_loss: 1.4139 - val_accuracy: 0.4761\n",
      "Epoch 744/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1871 - accuracy: 0.5497 - val_loss: 1.4072 - val_accuracy: 0.4799\n",
      "Epoch 745/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1652 - accuracy: 0.5589 - val_loss: 1.4193 - val_accuracy: 0.4769\n",
      "Epoch 746/1000\n",
      "296/296 [==============================] - 0s 759us/step - loss: 1.1603 - accuracy: 0.5597 - val_loss: 1.4056 - val_accuracy: 0.4676\n",
      "Epoch 747/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1703 - accuracy: 0.5513 - val_loss: 1.4282 - val_accuracy: 0.4719\n",
      "Epoch 748/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1811 - accuracy: 0.5494 - val_loss: 1.3981 - val_accuracy: 0.4816\n",
      "Epoch 749/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1930 - accuracy: 0.5548 - val_loss: 1.3867 - val_accuracy: 0.4723\n",
      "Epoch 750/1000\n",
      "296/296 [==============================] - 0s 720us/step - loss: 1.1777 - accuracy: 0.5549 - val_loss: 1.3972 - val_accuracy: 0.4761\n",
      "Epoch 751/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1851 - accuracy: 0.5499 - val_loss: 1.4391 - val_accuracy: 0.4664\n",
      "Epoch 752/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1894 - accuracy: 0.5468 - val_loss: 1.4083 - val_accuracy: 0.4579\n",
      "Epoch 753/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1864 - accuracy: 0.5487 - val_loss: 1.3869 - val_accuracy: 0.4816\n",
      "Epoch 754/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.1677 - accuracy: 0.5502 - val_loss: 1.3981 - val_accuracy: 0.4706\n",
      "Epoch 755/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1699 - accuracy: 0.5546 - val_loss: 1.3913 - val_accuracy: 0.4812\n",
      "Epoch 756/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1773 - accuracy: 0.5516 - val_loss: 1.4273 - val_accuracy: 0.4740\n",
      "Epoch 757/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1794 - accuracy: 0.5535 - val_loss: 1.4187 - val_accuracy: 0.4740\n",
      "Epoch 758/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1828 - accuracy: 0.5561 - val_loss: 1.4380 - val_accuracy: 0.4719\n",
      "Epoch 759/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1865 - accuracy: 0.5517 - val_loss: 1.3947 - val_accuracy: 0.4833\n",
      "Epoch 760/1000\n",
      "296/296 [==============================] - 0s 747us/step - loss: 1.1705 - accuracy: 0.5542 - val_loss: 1.4147 - val_accuracy: 0.4812\n",
      "Epoch 761/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1709 - accuracy: 0.5523 - val_loss: 1.4102 - val_accuracy: 0.4829\n",
      "Epoch 762/1000\n",
      "296/296 [==============================] - 0s 784us/step - loss: 1.1884 - accuracy: 0.5572 - val_loss: 1.3926 - val_accuracy: 0.4888\n",
      "Epoch 763/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1597 - accuracy: 0.5616 - val_loss: 1.4311 - val_accuracy: 0.4841\n",
      "Epoch 764/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1788 - accuracy: 0.5567 - val_loss: 1.4175 - val_accuracy: 0.4752\n",
      "Epoch 765/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1813 - accuracy: 0.5567 - val_loss: 1.4127 - val_accuracy: 0.4791\n",
      "Epoch 766/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1735 - accuracy: 0.5546 - val_loss: 1.3922 - val_accuracy: 0.4714\n",
      "Epoch 767/1000\n",
      "296/296 [==============================] - 0s 765us/step - loss: 1.1686 - accuracy: 0.5532 - val_loss: 1.3947 - val_accuracy: 0.4799\n",
      "Epoch 768/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1827 - accuracy: 0.5545 - val_loss: 1.3860 - val_accuracy: 0.4829\n",
      "Epoch 769/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1960 - accuracy: 0.5520 - val_loss: 1.4144 - val_accuracy: 0.4680\n",
      "Epoch 770/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1880 - accuracy: 0.5545 - val_loss: 1.3921 - val_accuracy: 0.4752\n",
      "Epoch 771/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1808 - accuracy: 0.5552 - val_loss: 1.3966 - val_accuracy: 0.4748\n",
      "Epoch 772/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1686 - accuracy: 0.5584 - val_loss: 1.3993 - val_accuracy: 0.4799\n",
      "Epoch 773/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1830 - accuracy: 0.5537 - val_loss: 1.4115 - val_accuracy: 0.4786\n",
      "Epoch 774/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1737 - accuracy: 0.5598 - val_loss: 1.4234 - val_accuracy: 0.4731\n",
      "Epoch 775/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1684 - accuracy: 0.5553 - val_loss: 1.4210 - val_accuracy: 0.4803\n",
      "Epoch 776/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1718 - accuracy: 0.5581 - val_loss: 1.4243 - val_accuracy: 0.4786\n",
      "Epoch 777/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1683 - accuracy: 0.5584 - val_loss: 1.4221 - val_accuracy: 0.4740\n",
      "Epoch 778/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1780 - accuracy: 0.5512 - val_loss: 1.4115 - val_accuracy: 0.4833\n",
      "Epoch 779/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1705 - accuracy: 0.5588 - val_loss: 1.4124 - val_accuracy: 0.4765\n",
      "Epoch 780/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1784 - accuracy: 0.5556 - val_loss: 1.4326 - val_accuracy: 0.4791\n",
      "Epoch 781/1000\n",
      "296/296 [==============================] - 0s 724us/step - loss: 1.1720 - accuracy: 0.5568 - val_loss: 1.4143 - val_accuracy: 0.4782\n",
      "Epoch 782/1000\n",
      "296/296 [==============================] - 0s 737us/step - loss: 1.1767 - accuracy: 0.5548 - val_loss: 1.4361 - val_accuracy: 0.4799\n",
      "Epoch 783/1000\n",
      "296/296 [==============================] - 0s 763us/step - loss: 1.1764 - accuracy: 0.5596 - val_loss: 1.3942 - val_accuracy: 0.4896\n",
      "Epoch 784/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1846 - accuracy: 0.5541 - val_loss: 1.4065 - val_accuracy: 0.4693\n",
      "Epoch 785/1000\n",
      "296/296 [==============================] - 0s 726us/step - loss: 1.1772 - accuracy: 0.5542 - val_loss: 1.3951 - val_accuracy: 0.4799\n",
      "Epoch 786/1000\n",
      "296/296 [==============================] - 0s 716us/step - loss: 1.1835 - accuracy: 0.5582 - val_loss: 1.3938 - val_accuracy: 0.4778\n",
      "Epoch 787/1000\n",
      "296/296 [==============================] - 0s 730us/step - loss: 1.1718 - accuracy: 0.5577 - val_loss: 1.4112 - val_accuracy: 0.4786\n",
      "Epoch 788/1000\n",
      "296/296 [==============================] - 0s 757us/step - loss: 1.1858 - accuracy: 0.5546 - val_loss: 1.4202 - val_accuracy: 0.4774\n",
      "Epoch 789/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1640 - accuracy: 0.5586 - val_loss: 1.3974 - val_accuracy: 0.4816\n",
      "Epoch 790/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.1676 - accuracy: 0.5575 - val_loss: 1.3953 - val_accuracy: 0.4786\n",
      "Epoch 791/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1684 - accuracy: 0.5545 - val_loss: 1.3936 - val_accuracy: 0.4812\n",
      "Epoch 792/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1659 - accuracy: 0.5543 - val_loss: 1.4060 - val_accuracy: 0.4769\n",
      "Epoch 793/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1857 - accuracy: 0.5547 - val_loss: 1.4116 - val_accuracy: 0.4774\n",
      "Epoch 794/1000\n",
      "296/296 [==============================] - 0s 725us/step - loss: 1.1802 - accuracy: 0.5557 - val_loss: 1.4106 - val_accuracy: 0.4744\n",
      "Epoch 795/1000\n",
      "296/296 [==============================] - 0s 739us/step - loss: 1.1729 - accuracy: 0.5579 - val_loss: 1.4047 - val_accuracy: 0.4740\n",
      "Epoch 796/1000\n",
      "296/296 [==============================] - 0s 718us/step - loss: 1.1673 - accuracy: 0.5557 - val_loss: 1.4220 - val_accuracy: 0.4702\n",
      "Epoch 797/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1563 - accuracy: 0.5585 - val_loss: 1.4074 - val_accuracy: 0.4719\n",
      "Epoch 798/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1838 - accuracy: 0.5575 - val_loss: 1.4196 - val_accuracy: 0.4736\n",
      "Epoch 799/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1684 - accuracy: 0.5596 - val_loss: 1.4316 - val_accuracy: 0.4786\n",
      "Epoch 800/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1730 - accuracy: 0.5524 - val_loss: 1.3872 - val_accuracy: 0.4752\n",
      "Epoch 801/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1778 - accuracy: 0.5566 - val_loss: 1.4010 - val_accuracy: 0.4693\n",
      "Epoch 802/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.1679 - accuracy: 0.5585 - val_loss: 1.4250 - val_accuracy: 0.4752\n",
      "Epoch 803/1000\n",
      "296/296 [==============================] - 0s 756us/step - loss: 1.1952 - accuracy: 0.5539 - val_loss: 1.3950 - val_accuracy: 0.4702\n",
      "Epoch 804/1000\n",
      "296/296 [==============================] - 0s 744us/step - loss: 1.1685 - accuracy: 0.5557 - val_loss: 1.4097 - val_accuracy: 0.4740\n",
      "Epoch 805/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1869 - accuracy: 0.5585 - val_loss: 1.4254 - val_accuracy: 0.4740\n",
      "Epoch 806/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1584 - accuracy: 0.5611 - val_loss: 1.4298 - val_accuracy: 0.4757\n",
      "Epoch 807/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1622 - accuracy: 0.5577 - val_loss: 1.4057 - val_accuracy: 0.4630\n",
      "Epoch 808/1000\n",
      "296/296 [==============================] - 0s 750us/step - loss: 1.1748 - accuracy: 0.5599 - val_loss: 1.4244 - val_accuracy: 0.4642\n",
      "Epoch 809/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1877 - accuracy: 0.5562 - val_loss: 1.4034 - val_accuracy: 0.4752\n",
      "Epoch 810/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.1817 - accuracy: 0.5496 - val_loss: 1.4046 - val_accuracy: 0.4837\n",
      "Epoch 811/1000\n",
      "296/296 [==============================] - 0s 721us/step - loss: 1.1900 - accuracy: 0.5524 - val_loss: 1.4065 - val_accuracy: 0.4752\n",
      "Epoch 812/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1699 - accuracy: 0.5620 - val_loss: 1.3992 - val_accuracy: 0.4727\n",
      "Epoch 813/1000\n",
      "296/296 [==============================] - 0s 738us/step - loss: 1.1638 - accuracy: 0.5625 - val_loss: 1.4005 - val_accuracy: 0.4841\n",
      "Epoch 814/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1622 - accuracy: 0.5601 - val_loss: 1.4203 - val_accuracy: 0.4731\n",
      "Epoch 815/1000\n",
      "296/296 [==============================] - 0s 748us/step - loss: 1.1586 - accuracy: 0.5661 - val_loss: 1.3882 - val_accuracy: 0.4765\n",
      "Epoch 816/1000\n",
      "296/296 [==============================] - 0s 755us/step - loss: 1.1676 - accuracy: 0.5575 - val_loss: 1.4154 - val_accuracy: 0.4752\n",
      "Epoch 817/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1584 - accuracy: 0.5641 - val_loss: 1.4028 - val_accuracy: 0.4731\n",
      "Epoch 818/1000\n",
      "296/296 [==============================] - 0s 732us/step - loss: 1.1644 - accuracy: 0.5553 - val_loss: 1.4216 - val_accuracy: 0.4731\n",
      "Epoch 819/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1715 - accuracy: 0.5540 - val_loss: 1.4175 - val_accuracy: 0.4774\n",
      "Epoch 820/1000\n",
      "296/296 [==============================] - 0s 719us/step - loss: 1.1889 - accuracy: 0.5528 - val_loss: 1.3747 - val_accuracy: 0.4846\n",
      "Epoch 821/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1671 - accuracy: 0.5580 - val_loss: 1.4032 - val_accuracy: 0.4774\n",
      "Epoch 822/1000\n",
      "296/296 [==============================] - 0s 753us/step - loss: 1.1829 - accuracy: 0.5546 - val_loss: 1.4195 - val_accuracy: 0.4782\n",
      "Epoch 823/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1755 - accuracy: 0.5554 - val_loss: 1.4135 - val_accuracy: 0.4824\n",
      "Epoch 824/1000\n",
      "296/296 [==============================] - 0s 749us/step - loss: 1.1745 - accuracy: 0.5574 - val_loss: 1.4096 - val_accuracy: 0.4854\n",
      "Epoch 825/1000\n",
      "296/296 [==============================] - 0s 735us/step - loss: 1.1631 - accuracy: 0.5600 - val_loss: 1.4251 - val_accuracy: 0.4786\n",
      "Epoch 826/1000\n",
      "296/296 [==============================] - 0s 734us/step - loss: 1.1818 - accuracy: 0.5480 - val_loss: 1.4154 - val_accuracy: 0.4765\n",
      "Epoch 827/1000\n",
      "296/296 [==============================] - 0s 740us/step - loss: 1.1789 - accuracy: 0.5560 - val_loss: 1.4355 - val_accuracy: 0.4807\n",
      "Epoch 828/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.1644 - accuracy: 0.5577 - val_loss: 1.4190 - val_accuracy: 0.4744\n",
      "Epoch 829/1000\n",
      "296/296 [==============================] - 0s 729us/step - loss: 1.1685 - accuracy: 0.5606 - val_loss: 1.4292 - val_accuracy: 0.4685\n",
      "Epoch 830/1000\n",
      "296/296 [==============================] - 0s 731us/step - loss: 1.1669 - accuracy: 0.5561 - val_loss: 1.4180 - val_accuracy: 0.4795\n",
      "Epoch 831/1000\n",
      "296/296 [==============================] - 0s 736us/step - loss: 1.1757 - accuracy: 0.5589 - val_loss: 1.4254 - val_accuracy: 0.4841\n",
      "Epoch 832/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1775 - accuracy: 0.5530 - val_loss: 1.4082 - val_accuracy: 0.4829\n",
      "Epoch 833/1000\n",
      "296/296 [==============================] - 0s 752us/step - loss: 1.1687 - accuracy: 0.5564 - val_loss: 1.4033 - val_accuracy: 0.4820\n",
      "Epoch 834/1000\n",
      "296/296 [==============================] - 0s 733us/step - loss: 1.1794 - accuracy: 0.5589 - val_loss: 1.4159 - val_accuracy: 0.4774\n",
      "Epoch 835/1000\n",
      "296/296 [==============================] - 0s 771us/step - loss: 1.1767 - accuracy: 0.5563 - val_loss: 1.4331 - val_accuracy: 0.4850\n",
      "Epoch 836/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.1831 - accuracy: 0.5522 - val_loss: 1.4030 - val_accuracy: 0.4702\n",
      "Epoch 837/1000\n",
      "296/296 [==============================] - 0s 777us/step - loss: 1.1604 - accuracy: 0.5618 - val_loss: 1.4179 - val_accuracy: 0.4791\n",
      "Epoch 838/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.1726 - accuracy: 0.5594 - val_loss: 1.4276 - val_accuracy: 0.4757\n",
      "Epoch 839/1000\n",
      "296/296 [==============================] - 0s 751us/step - loss: 1.1678 - accuracy: 0.5548 - val_loss: 1.4086 - val_accuracy: 0.4736\n",
      "Epoch 840/1000\n",
      "296/296 [==============================] - 0s 727us/step - loss: 1.1755 - accuracy: 0.5587 - val_loss: 1.4258 - val_accuracy: 0.4795\n",
      "Epoch 841/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1710 - accuracy: 0.5598 - val_loss: 1.4356 - val_accuracy: 0.4672\n",
      "Epoch 842/1000\n",
      "296/296 [==============================] - 0s 776us/step - loss: 1.1752 - accuracy: 0.5566 - val_loss: 1.4316 - val_accuracy: 0.4782\n",
      "Epoch 843/1000\n",
      "296/296 [==============================] - 0s 780us/step - loss: 1.1742 - accuracy: 0.5561 - val_loss: 1.4160 - val_accuracy: 0.4778\n",
      "Epoch 844/1000\n",
      "296/296 [==============================] - 0s 774us/step - loss: 1.1724 - accuracy: 0.5571 - val_loss: 1.4258 - val_accuracy: 0.4693\n",
      "Epoch 845/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1763 - accuracy: 0.5506 - val_loss: 1.4049 - val_accuracy: 0.4850\n",
      "Epoch 846/1000\n",
      "296/296 [==============================] - 0s 762us/step - loss: 1.1707 - accuracy: 0.5597 - val_loss: 1.4026 - val_accuracy: 0.4807\n",
      "Epoch 847/1000\n",
      "296/296 [==============================] - 0s 766us/step - loss: 1.1790 - accuracy: 0.5559 - val_loss: 1.4342 - val_accuracy: 0.4748\n",
      "Epoch 848/1000\n",
      "296/296 [==============================] - 0s 745us/step - loss: 1.1641 - accuracy: 0.5564 - val_loss: 1.4241 - val_accuracy: 0.4706\n",
      "Epoch 849/1000\n",
      "296/296 [==============================] - 0s 728us/step - loss: 1.1625 - accuracy: 0.5615 - val_loss: 1.4409 - val_accuracy: 0.4752\n",
      "Epoch 850/1000\n",
      "296/296 [==============================] - 0s 768us/step - loss: 1.1761 - accuracy: 0.5550 - val_loss: 1.4089 - val_accuracy: 0.4702\n",
      "Epoch 851/1000\n",
      "296/296 [==============================] - 0s 742us/step - loss: 1.1900 - accuracy: 0.5529 - val_loss: 1.4180 - val_accuracy: 0.4791\n",
      "Epoch 852/1000\n",
      "296/296 [==============================] - 0s 743us/step - loss: 1.1607 - accuracy: 0.5640 - val_loss: 1.4256 - val_accuracy: 0.4736\n",
      "Epoch 853/1000\n",
      "296/296 [==============================] - 0s 741us/step - loss: 1.1709 - accuracy: 0.5596 - val_loss: 1.4239 - val_accuracy: 0.4706\n",
      "Epoch 854/1000\n",
      "  1/296 [..............................] - ETA: 0s - loss: 1.0303 - accuracy: 0.5938"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 5e-3\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(X_train_t.shape[1],)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(y_train_t.shape[1], activation='softmax'))\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer=SGD(learning_rate=learning_rate, momentum=0.8),metrics=['accuracy'])\n",
    "model_fit = model.fit(X_train_t, y_train_t, epochs=num_epochs, validation_data=(X_test_t, y_test_t), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Acoustic/Folk',\n",
       " 1: 'Alt_Music',\n",
       " 2: 'Blues',\n",
       " 3: 'Bollywood',\n",
       " 4: 'Country',\n",
       " 5: 'HipHop',\n",
       " 6: 'Indie Alt',\n",
       " 7: 'Instrumental',\n",
       " 8: 'Metal',\n",
       " 9: 'Pop',\n",
       " 10: 'Rock'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('submission.csv')\n",
    "#imprime las columnas\n",
    "\n",
    "genero_labels = {}\n",
    "for columna in sub.columns:\n",
    "    split = columna.rsplit('_', 1)\n",
    "    genero_labels[int(split[1])] = split[0]\n",
    "genero_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Acoustic/Folk</th>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alt_Music</th>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.092166</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blues</th>\n",
       "      <td>0.320225</td>\n",
       "      <td>0.280788</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bollywood</th>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.637363</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HipHop</th>\n",
       "      <td>0.395062</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indie Alt</th>\n",
       "      <td>0.239336</td>\n",
       "      <td>0.237089</td>\n",
       "      <td>0.238208</td>\n",
       "      <td>426.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instrumental</th>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metal</th>\n",
       "      <td>0.508251</td>\n",
       "      <td>0.473846</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pop</th>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.251208</td>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rock</th>\n",
       "      <td>0.352846</td>\n",
       "      <td>0.328788</td>\n",
       "      <td>0.340392</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.340669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.437041</td>\n",
       "      <td>0.424114</td>\n",
       "      <td>0.429141</td>\n",
       "      <td>2363.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.355440</td>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.347139</td>\n",
       "      <td>2363.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision    recall  f1-score      support\n",
       "Acoustic/Folk   0.707692  0.575000  0.634483    80.000000\n",
       "Alt_Music       0.063898  0.092166  0.075472   217.000000\n",
       "Blues           0.320225  0.280788  0.299213   203.000000\n",
       "Bollywood       0.604167  0.674419  0.637363    43.000000\n",
       "Country         0.482759  0.466667  0.474576    30.000000\n",
       "HipHop          0.395062  0.351648  0.372093    91.000000\n",
       "Indie Alt       0.239336  0.237089  0.238208   426.000000\n",
       "Instrumental    0.892473  0.922222  0.907104    90.000000\n",
       "Metal           0.508251  0.473846  0.490446   325.000000\n",
       "Pop             0.240741  0.262626  0.251208   198.000000\n",
       "Rock            0.352846  0.328788  0.340392   660.000000\n",
       "accuracy        0.340669  0.340669  0.340669     0.340669\n",
       "macro avg       0.437041  0.424114  0.429141  2363.000000\n",
       "weighted avg    0.355440  0.340669  0.347139  2363.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Mostrar métricas\n",
    "y_test_labeled = y_test.map(genero_labels)\n",
    "y_pred_labeled = pd.Series(y_pred).map(genero_labels)\n",
    "\n",
    "classification_metrics_labeled = classification_report(y_test_labeled, y_pred_labeled, output_dict=True)\n",
    "classification_metrics_labeled_df = pd.DataFrame(classification_metrics_labeled).transpose()\n",
    "classification_metrics_labeled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f4d798b5270>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
